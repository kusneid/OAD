Лаба 1. Загрузка исходных данных и техническое выравнивание (RAW → Staging)  
Студент выбирает датасет и фиксирует, какие наборы данных он использует (файлы/таблицы), в каком формате (CSV/JSON/Parquet) и откуда берётся время (timestamp/дата/период). Минимальное требование: в данных должно быть время и хотя бы один стабильный идентификатор сущности (например, id объекта/пользователя/товара/документа), плюс хотя бы один набор записей, который выглядит как “события/наблюдения/операции” (много строк во времени). Исходные файлы кладутся в RAW-слой на S3/MinIO или локально в структуру Data Lake (одна папка = один датасет, дальше разбиение по партициям dt, если это возможно). Дальше делается Spark batch job, который читает RAW, приводит типы (числа/строки/даты), нормализует время в один тип (например, DateTime), приводит имена колонок к единообразному стилю (snake_case), удаляет технический мусор (пустые строки, очевидно битые timestamps), фиксирует правило дедупликации (по какому ключу запись считается уникальной), и пишет результат в staging-слой. Staging хранится либо в Postgres (staging_* таблицы), либо как Parquet в staging-папке (если дальше удобнее читать Spark’ом), но формат должен быть воспроизводимым: один и тот же запуск на одном dt даёт один и тот же результат без дублей. Проверка на защите: показывается, что staging-таблицы/файлы создаются, что типы корректные, что время читается и фильтруется, что дедупликация работает.

Лаба 2. Построение Data Vault: Hubs / Links / Satellites + историзация  
Из staging-слоя студент выделяет сущности и отношения. Требование по модели: минимум два Hub’а (две сущности с бизнес-ключом), минимум один Link (отношение между Hub’ами или “событийная связь”), и Satellites для атрибутов. Для каждого Hub фиксируется бизнес-ключ (что в данных является естественным идентификатором, по которому сущность определяется), и вводится технический ключ (hash key или surrogate key — выбор фиксируется и дальше не меняется). Для Link фиксируется состав участников (какие hub_hk входят) и, если нужно, ключ самой связи. Для Satellites фиксируется набор атрибутов и логика изменений: satellite хранит версии во времени загрузки, новая строка появляется только когда атрибуты реально изменились (через hashdiff от набора атрибутов или эквивалентное правило). Везде добавляется минимум: load_ts (время загрузки), record_source (откуда пришло), и правила идемпотентной загрузки (повторный прогон не плодит одинаковые версии). Если в данных есть изменения задним числом или периодические “срезы”, студент обязан выбрать и описать, какое время считается “временем факта” (event_time) и какое — “временем загрузки” (load_ts), и что именно историзируется. Реализация допускается в Postgres или ClickHouse, но таблицы должны быть физически созданы и заполнены. Проверка на защите: показывается схема DV на конкретном датасете (какие hubs/links/sats), демонстрируется, что satellites реально хранят историю (можно показать две версии по одному hk), и что повторный запуск не размножает одинаковые записи.

Лаба 3. DV → аналитическая модель потребления: факты/измерения и широкие витрины  
На базе DV студент строит аналитический слой, который удобно потреблять в BI и ML. Требование: определить аналитические grain’ы (что означает строка в витрине) и собрать минимум две витрины в потребительском виде. Одна витрина должна быть “по времени” (например, entity×day или day×category), вторая — “по сущности” (например, entity-level с агрегатами за период или последним состоянием). Витрины должны быть широкими: одна таблица включает сразу набор метрик и атрибутов, чтобы по ней можно было строить несколько графиков без тяжёлых join. Источник истины для витрин — DV: факты собираются из links (как ось событий/отношений) и satellites (как атрибуты и временные поля), измерения собираются из hubs + актуальных/исторических satellites по выбранной логике. Если атрибуты сущности меняются во времени и это влияет на аналитику, студент обязан выбрать: витрина “as-of” (атрибуты на момент события) или витрина “current” (атрибуты актуальные сейчас), и реализовать это последовательно. Реализация трансформаций делается либо чистым SQL, либо dbt (рекомендуется, но не обязателен), но должна быть воспроизводима одним запуском. Проверка на защите: показывается, что у витрины чётко задан ключ уникальности (grain), что нет раздувания строк при сборке, и что витрины строятся повторяемо из DV.

Лаба 4. OLAP в ClickHouse: корректные запросы, физическая модель, pre-aggregations  
Эта лаба целиком про то, как на реальных данных писать аналитические запросы и подгонять хранение под них. Студент выбирает набор метрик и срезов, которые естественны для его датасета, но набор обязан включать: метрику активности во времени (аналог DAU/число активных сущностей/число событий), метрику “объёма” (сумма/количество/длительность), группировку по категории (тип события/класс/жанр/статус), и хотя бы один анализ “кохортного” типа, если в данных есть первый момент появления сущности (например, first_seen_dt). Далее студент реализует эти расчёты SQL-ом в ClickHouse на витринах/фактах из Лабы 3. После этого студент настраивает физическую модель минимум для одной крупной таблицы: задаёт PARTITION BY (по времени) и ORDER BY (по тем полям, по которым реально фильтруют/группируют), и объясняет выбор через характер запросов. Затем строится pre-aggregation слой: отдельные таблицы/представления, которые BI будет читать напрямую (дневные KPI, таблица retention/cohort, таблица кумулятивной метрики по возрасту когорты или аналог). Проверка на защите: показываются сами запросы и результат, показывается схема таблиц с ключами и партициями, демонстрируется, что BI-таблицы считаются как отдельный слой, а не “случайно в одном запросе”.

Лаба 5. BI: Superset/Power BI/Tableau поверх подготовленных BI-таблиц  
Студент подключает BI-инструмент к ClickHouse и строит один полноценный дашборд на pre-aggregations из Лабы 4. Требование: дашборд должен содержать временной тренд активности/объёма, разрез по категории (top-N или распределение), и визуализацию кохортного типа (heatmap/кривые), если этот анализ был реализован. Важное требование: BI читает готовые bi-таблицы и не делает тяжёлых многотабличных вычислений “на лету” как основную логику. На защите студент показывает, что все графики опираются на конкретные таблицы/запросы, и что фильтры по времени работают корректно.

Лаба 6. Feature Engineering: построение feature sets из аналитических витрин  
Студент выбирает ML-задачу, которая соответствует его данным: прогноз временного ряда (если есть динамика по дням), scoring/регрессия (если есть цель на сущности), или кластеризация (если цель не задана). Дальше строится feature set как таблица с фиксированным grain: либо entity×dt (для forecasting/скоринга по времени), либо entity (для сегментации). Требования к фичам: обязательны лаги (минимум 1, 7 и 30 периодов, если период дневной; если другой период — аналогично), rolling-агрегаты (скользящие суммы/средние/счётчики), и календарные признаки (день недели/месяц/сезонность) при наличии регулярного времени. Важно: признаки должны использовать только прошлую информацию относительно точки предсказания (без leakage), и разбиение train/validation делается по времени, если задача временная. Feature set сохраняется как отдельная таблица в ClickHouse или как Parquet в S3/MinIO (выбор фиксируется), и строится воспроизводимым кодом. Проверка на защите: показывается схема feature set, логика расчёта лагов/окон, и что train/validation действительно разделены во времени.

Лаба 7. Прикладное ML и публикация результатов обратно в слой данных  
Студент обучает модели на feature set из Лабы 6. Для forecasting/регрессии требуется минимум две модели из классического стека sklearn (например, линейная + дерево/лес/градиентный бустинг); для кластеризации — k-means плюс интерпретация кластеров через агрегаты по кластерам. Оценка качества обязательна: для прогнозов/регрессии — MAE и MAPE на backtesting (по временным фолдам или отложенному периоду), для кластеризации — минимум sanity-интерпретация (различимость кластеров по ключевым метрикам, размеры кластеров, стабильность признаков). Результаты записываются обратно в ClickHouse: отдельная таблица для прогнозов/скоринга и отдельная таблица для сегментов/кластеров. Эти результаты должны быть готовы для BI: по ним можно построить “факт vs прогноз” (если есть факт) и распределение сегментов. Проверка на защите: показываются таблицы результатов, метрики качества, и пример потребления в SQL/BI.

Лаба 8. Оркестрация локально: регулярный пайплайн “extract → features → predict → publish”  
Эта лаба делает процесс воспроизводимым и похожим на production workflow, но без тяжёлой инфраструктуры. Реализация допускается в двух эквивалентных вариантах: Airflow в лёгком режиме (SequentialExecutor, запуск через CLI без standalone) или Python pipeline с явными шагами и зависимостями. Пайплайн должен принимать дату/период расчёта и run_id, последовательно выполнять: извлечение нужных витрин/bi-таблиц, построение/обновление feature set, запуск модели/предсказаний, публикацию результатов в таблицы, и обновление BI-таблицы “forecast vs actual” или аналога. Должна быть логика quality gate: если качество на валидации хуже заданного порога или входные данные неполные за период, публикация результата в потребительский слой не происходит (остаются только промежуточные артефакты или пишется запись о неуспешном запуске в отдельную таблицу алертов). Проверка на защите: демонстрируется запуск пайплайна за конкретную дату, видны созданные/обновлённые таблицы, и показано поведение при “плохом” сценарии (gate срабатывает).
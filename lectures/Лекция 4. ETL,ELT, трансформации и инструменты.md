## Слайд 1. Роль трансформаций в аналитической платформе

Трансформации — это совокупность операций, которые изменяют структуру, содержание и семантику данных после их загрузки из источников и до момента потребления аналитическими системами. Они не являются побочным этапом пайплайна, а формируют саму ценность данных для аналитики. Без трансформаций данные остаются набором несвязанных событий, записей и файлов, не пригодных для устойчивого анализа.

Ключевая функция трансформаций — превратить технические данные в аналитические. Источники генерируют данные в формате, удобном для транзакционных систем, логирования или передачи по сети, но не для аналитических запросов. Трансформации устраняют это несоответствие, формируя согласованные структуры, приводя типы данных, синхронизируя временные оси и объединяя информацию из разных источников.

Трансформации также задают правила интерпретации данных. На этом этапе определяется, какие записи считаются валидными, как трактуются пропуски, какие события объединяются в одну сущность, а какие считаются независимыми. Это означает, что трансформации напрямую влияют на значения метрик и аналитических показателей.

Важно отделять трансформации от ingestion. Ingestion отвечает за доставку данных в платформу и не должен содержать сложной логики. Любая логика, влияющая на аналитический смысл данных, должна находиться в трансформационном слое, чтобы быть воспроизводимой и управляемой.

---

## Слайд 2. Место трансформаций между слоями данных

Трансформации распределены по всей цепочке слоёв данных и выполняют разные функции в зависимости от уровня. Между слоями RAW, Staging, Aggregated и витринами данные проходят последовательные изменения, каждое из которых решает строго определённый класс задач.

На уровне перехода из RAW в Staging трансформации носят технический характер. Здесь данные приводятся к стабильной структуре: парсятся форматы, выравниваются типы, устраняются очевидные ошибки источников. Цель этого этапа — получить данные, пригодные для дальнейшей систематической обработки, но без внедрения бизнес-смыслов.

Переход из Staging в Aggregated связан с появлением семантики. На этом этапе данные объединяются между собой, добавляются вычисляемые поля, формируются согласованные определения показателей. Здесь начинается аналитическая логика, но она всё ещё ориентирована на универсальное использование, а не на конкретные отчёты.

Финальный этап трансформаций — сборка витрин. Это целенаправленное преобразование агрегированных данных в широкие, денормализованные таблицы, оптимизированные под чтение и анализ. На этом уровне трансформации подчинены требованиям скорости, стабильности и удобства потребителей данных.

Таким образом, трансформации — это не единый шаг, а сквозной процесс, который сопровождает данные на всём пути от сырого состояния до аналитического продукта.

---

## Слайд 3. Типы задач, которые решаются трансформациями

Трансформации решают несколько принципиально разных классов задач, которые невозможно свести к одной универсальной операции. Каждый тип задач требует своих подходов и инструментов, но все они должны быть согласованы между собой.

Первая группа — структурные задачи. Они связаны с приведением данных к единой форме: согласование схем, типов данных, форматов дат и времени, идентификаторов. Без этого невозможно выполнять корректные объединения и агрегации.

Вторая группа — задачи качества данных. Источники часто содержат дубликаты, пропуски, некорректные значения или противоречивые записи. Трансформации определяют правила обработки таких ситуаций: какие записи исключаются, какие исправляются, а какие сохраняются с пометками.

Третья группа — семантические задачи. Они отвечают за смысл данных: вычисление производных показателей, объединение событий в более крупные сущности, интерпретацию статусов и состояний. На этом этапе данные начинают соответствовать аналитическим понятиям.

Четвёртая группа — агрегационные задачи. Это расчёт сумм, средних, оконных метрик, временных срезов. Они позволяют сократить объём данных и подготовить их к быстрому анализу.

Все эти задачи выполняются не изолированно, а в виде связанной цепочки, где ошибки на ранних этапах неизбежно искажают результат на поздних.

---

## Слайд 4. ETL как исторический подход

ETL сформировался в условиях, когда аналитические системы имели жёсткие ограничения по хранению и вычислениям. Хранилища данных были дорогими, масштабирование — сложным, а перерасчёт данных — ресурсоёмким и рискованным.

В этой модели трансформации выполнялись до загрузки в аналитическое хранилище. Данные очищались, агрегировались и приводились к конечной схеме ещё на этапе загрузки. В хранилище попадали только те данные, которые считались готовыми для анализа.

Такой подход требовал заранее фиксировать схему и бизнес-логику. Любое изменение требований означало изменение ETL-пайплайна и повторную загрузку данных из источников. Это делало систему негибкой и зависимой от первоначальных решений.

ETL хорошо работал в условиях стабильных источников и фиксированных отчётов, но плохо переносил изменения. Ошибка в трансформации на этапе загрузки часто приводила к потере данных, которые невозможно было восстановить без повторного извлечения из источника.

Несмотря на это, ETL долгое время оставался стандартом из-за отсутствия альтернативных архитектурных возможностей.

---

## Слайд 5. ELT как современная доминирующая модель

ELT возник как ответ на ограничения ETL и стал возможен благодаря изменению технологической базы. Ключевое отличие ELT заключается в том, что данные сначала загружаются в аналитическое хранилище или data lake в максимально исходном виде, а трансформации выполняются уже после загрузки.

Это позволяет сохранять полную версию исходных данных без потерь. Любые изменения логики обработки не требуют повторного обращения к источникам, поскольку данные уже находятся внутри платформы. Трансформации становятся повторяемыми и управляемыми.

ELT переносит основную сложность из ingestion в трансформационный слой. Ingestion упрощается и стабилизируется, а вся аналитическая логика концентрируется в одном месте. Это облегчает тестирование, версионирование и сопровождение.

Дополнительным эффектом ELT является возможность параллельной разработки. Разные команды могут строить собственные трансформации поверх одних и тех же загруженных данных, не вмешиваясь в процесс ingestion.

ELT не устраняет трансформации, а делает их центральным объектом управления и развития аналитической платформы.

---

## Слайд 6. ETL vs ELT как инженерный выбор

Противопоставление ETL и ELT не является вопросом правильного или неправильного подхода. Это инженерный выбор, зависящий от требований к системе, ограничений инфраструктуры и характера данных.

ETL уменьшает объём данных, попадающих в аналитическое хранилище, и может быть оправдан при жёстких ограничениях на хранение или требованиях к минимальной задержке. Однако он усложняет эволюцию логики и повышает риск необратимых ошибок.

ELT увеличивает требования к вычислительным ресурсам, но обеспечивает гибкость и воспроизводимость. Он лучше подходит для аналитических сценариев, где логика часто меняется, а данные используются повторно в разных контекстах.

На практике чаще используется гибридная модель. Минимальные трансформации выполняются до загрузки для обеспечения технической корректности, а основная логика реализуется в ELT-стиле поверх сохранённых данных.

Выбор между ETL и ELT должен основываться на анализе компромиссов между стоимостью, сложностью сопровождения и устойчивостью системы к изменениям, а не на следовании моде или терминологии.

---
## Слайд 7. Гибридные схемы ETL + ELT в реальных системах

Чистые ETL или ELT архитектуры практически не встречаются в промышленных системах. Почти всегда используется гибридный подход, в котором разные типы трансформаций выполняются на разных этапах пайплайна. Причина в том, что требования к надёжности ingestion и требования к аналитической гибкости конфликтуют между собой.

На этапе ingestion обычно выполняются минимальные ETL-преобразования. Они направлены не на аналитику, а на техническую стабилизацию данных: базовая валидация формата, приведение кодировок, контроль допустимых размеров сообщений, добавление технических атрибутов загрузки. Эти операции нужны, чтобы данные вообще могли быть сохранены и обработаны дальше.

Основная аналитическая логика реализуется в ELT-части. После загрузки данных в RAW или Staging слой все содержательные трансформации выполняются поверх уже сохранённых данных. Это позволяет свободно менять логику, пересчитывать результаты и восстанавливать состояние при ошибках.

Гибридная модель снимает главный риск чистого ETL — потерю данных из-за ошибки трансформации — и при этом не перекладывает всю ответственность на downstream-слои. Такая схема считается наиболее устойчивой при долгосрочной эксплуатации.

---

## Слайд 8. Структурные трансформации

Структурные трансформации отвечают за приведение данных к формату, пригодному для систематической обработки. Источники редко согласованы между собой: одинаковые сущности могут иметь разные типы, названия полей, форматы дат и идентификаторов. Без устранения этих различий дальнейшие преобразования становятся невозможными или некорректными.

К типичным структурным трансформациям относятся:

- приведение типов данных к фиксированным аналитическим типам
    
- унификация форматов времени и временных зон
    
- нормализация идентификаторов и ключей
    
- стабилизация схемы (фиксированный набор колонок)
    

Эти операции выполняются как можно раньше, чаще всего на границе RAW → Staging. При этом они не должны добавлять аналитический смысл. Их задача — сделать данные технически совместимыми, а не «понятными бизнесу».

Пример минимальной структурной трансформации:

```python
df = df.withColumn("event_time", to_timestamp("event_time")) \
       .withColumn("user_id", col("user_id").cast("long"))
```

Ошибкой является включение бизнес-логики в этот этап. Любое правило, влияющее на интерпретацию данных, должно быть вынесено на следующий уровень трансформаций.

---

## Слайд 9. Качественные трансформации

Качественные трансформации направлены на контроль и улучшение надёжности данных. Источники данных почти всегда содержат дефекты: дубликаты, пропущенные значения, некорректные диапазоны, частично записанные события. Эти проблемы не являются исключениями — они являются нормой.

На этом этапе определяются правила, по которым система реагирует на дефекты:

- удаление или сохранение дубликатов
    
- обработка пропусков (заполнение, игнорирование, маркировка)
    
- фильтрация заведомо некорректных значений
    
- выделение брака в отдельные наборы данных
    

Ключевой момент заключается в том, что решения о качестве данных должны быть формализованы. Нельзя «интуитивно» чистить данные в разных местах пайплайна — это приводит к расхождению результатов.

Пример простой дедупликации по ключу и времени:

```sql
ROW_NUMBER() OVER (
  PARTITION BY event_id
  ORDER BY ingestion_time DESC
) = 1
```

Качественные трансформации не делают данные «красивыми», они делают их предсказуемыми и контролируемыми.

---

## Слайд 10. Семантические трансформации

Семантические трансформации формируют аналитический смысл данных. На этом этапе сырые записи начинают соответствовать аналитическим сущностям и показателям. Здесь появляются вычисляемые поля, интерпретации статусов и связи между разными наборами данных.

Типичные задачи семантических трансформаций:

- объединение событий в логические сущности
    
- вычисление производных показателей
    
- интерпретация состояний и переходов
    
- согласование терминов и определений
    

Важно, что семантическая логика должна быть централизована. Если одно и то же правило реализуется в нескольких местах, система быстро теряет согласованность. Именно поэтому этот этап часто реализуется в виде управляемых моделей трансформаций.

Пример семантического поля:

```sql
CASE
  WHEN status_code IN (200, 201) THEN 'success'
  ELSE 'error'
END AS event_status
```

Ошибки на этом уровне наиболее опасны, поскольку они не ломают данные технически, но искажают аналитические выводы.

---

## Слайд 11. Агрегационные трансформации

Агрегационные трансформации отвечают за сокращение объёма данных и подготовку их к аналитическому потреблению. Они включают группировки, оконные вычисления и расчёт производных временных показателей.

В аналитических системах агрегации выполняются не только для ускорения запросов, но и для фиксации определений метрик. Один и тот же показатель должен вычисляться одинаково во всех отчётах и моделях.

Примеры агрегационных задач:

- расчёт дневных и часовых показателей
    
- скользящие окна
    
- накопительные метрики
    
- предварительные суммы и счётчики
    

Пример оконной агрегации:

```sql
AVG(value) OVER (
  PARTITION BY user_id
  ORDER BY event_date
  ROWS BETWEEN 6 PRECEDING AND CURRENT ROW
)
```

Агрегации часто выполняются на уровне Aggregated слоя, но их результаты могут использоваться при сборке витрин и ML-признаков.

---

## Слайд 12. Технические трансформации и метаданные загрузки

Помимо структурных, качественных, семантических и агрегационных преобразований, в аналитических пайплайнах всегда присутствует слой технических трансформаций. Эти операции не влияют напрямую на бизнес-смысл данных, но обеспечивают управляемость, воспроизводимость и контроль пайплайна.

К техническим трансформациям относятся добавление метаданных загрузки: временных меток обработки, идентификаторов батчей, источников данных и версий логики. Эти поля позволяют отслеживать происхождение каждой записи и понимать, в каком контексте она была сформирована.

Типичные технические поля включают:

- время загрузки в систему
    
- время начала и окончания обработки
    
- идентификатор пайплайна или DAG
    
- версию трансформационной логики
    
- признак актуальности записи
    

Пример добавления технических атрибутов при трансформации:

`SELECT   *,   current_timestamp AS load_ts,   '{{ ds }}' AS batch_date FROM staging_table`

Технические трансформации критичны для отладки и поддержки. Без них невозможно корректно выполнять backfill, анализировать ошибки и сравнивать результаты разных запусков пайплайна.

Этот слой также используется для контроля согласованности данных между слоями и для построения мониторинга качества. Он связывает трансформационную логику с процессом исполнения и делает аналитическую систему наблюдаемой.

---
## Слайд 13. Инкрементальные трансформации

Инкрементальные трансформации предназначены для обработки только той части данных, которая появилась или изменилась с момента предыдущего запуска. Их основная цель — сократить объём вычислений и время выполнения пайплайна без потери корректности результата. В аналитических системах абсолютное большинство трансформаций стремятся быть инкрементальными, но это далеко не всегда возможно.

Инкрементальность опирается на наличие устойчивого критерия «новизны» данных. Чаще всего это временная метка события или загрузки, реже — монотонно растущий идентификатор. Этот критерий должен быть надёжным: если данные приходят с задержкой или переигрываются источником, простая фильтрация по времени приводит к потере записей.

Инкрементальные трансформации хорошо работают для операций, не зависящих от будущих данных. Например, добавление новых строк, локальные агрегации по дате, накопительные счётчики. Они плохо применимы к логике, где значение строки может измениться задним числом.

Проектирование инкрементальных трансформаций всегда начинается с ответа на вопрос: какие данные могут измениться после первоначальной загрузки и как далеко во времени это может происходить.

---

## Слайд 14. Full refresh и backfill

Полный пересчёт (full refresh) — это выполнение трансформации заново на всём объёме данных. Несмотря на высокую стоимость, он остаётся необходимым инструментом в аналитических системах. Некоторые типы логики невозможно корректно поддерживать инкрементально.

Backfill представляет собой частичный пересчёт исторического диапазона данных. Он используется, когда обнаружена ошибка в логике, изменилось определение показателя или источник начал присылать корректные данные задним числом. В отличие от полного пересчёта, backfill ограничен по периоду.

Главная сложность backfill — согласованность. При пересчёте исторических данных необходимо гарантировать, что новые значения не конфликтуют с уже рассчитанными периодами. Это требует аккуратного управления партициями, версиями данных и временем публикации результатов.

Хорошо спроектированная система трансформаций предполагает, что full refresh и backfill — это не аварийные процедуры, а штатные сценарии, которые можно выполнить контролируемо и воспроизводимо.

---

## Слайд 15. Идемпотентность трансформаций

Идемпотентность означает, что повторное выполнение трансформации с теми же входными данными приводит к тому же результату. Это фундаментальное свойство надёжных аналитических пайплайнов. Без него любой сбой или повторный запуск создаёт риск дублирования или искажения данных.

Идемпотентность достигается за счёт явного управления состоянием. Результаты трансформации либо полностью пересобираются, либо записываются в заранее очищенную область, либо обновляются по детерминированным ключам. Недопустима логика вида «дописать поверх предыдущего результата без контроля».

Особую сложность идемпотентность представляет для инкрементальных трансформаций. Здесь требуется чёткое разделение данных по временным или логическим срезам и строгие правила перезаписи этих срезов.

Идемпотентность позволяет безопасно автоматизировать пайплайны, выполнять ретраи, масштабировать вычисления и не опасаться побочных эффектов от повторных запусков.

---

## Слайд 16. Где исполняются трансформации

Место исполнения трансформаций определяет их производительность, сложность сопровождения и границы ответственности. В аналитических системах трансформации могут выполняться в распределённых вычислительных движках, внутри аналитических баз данных или в специализированных слоях управления логикой.

Распределённые движки применяются для тяжёлых преобразований: больших джоинов, сложных оконных вычислений, переработки массивных наборов данных. Они позволяют масштабировать вычисления горизонтально и работать с данными, которые не помещаются в память одного узла.

Аналитические базы данных эффективно справляются с агрегациями и финальными преобразованиями, ориентированными на чтение. Однако перенос в них сложной логики трансформаций усложняет контроль и повторное использование этой логики.

Разделение исполнения трансформаций по уровням позволяет оптимально использовать ресурсы и избежать концентрации всей логики в одном месте, что снижает риск неконтролируемой деградации системы.

---

## Слайд 17. Инструменты управления трансформациями

Трансформации в аналитических системах должны управляться как полноценный программный продукт. Это означает наличие версионирования, зависимости между шагами, контролируемого исполнения и документирования логики.

Инструменты управления трансформациями позволяют описывать цепочки преобразований декларативно, фиксировать их зависимости и обеспечивать согласованный порядок выполнения. Они делают явными связи между слоями данных и конечными артефактами.

Ключевая ценность таких инструментов заключается не в самом выполнении вычислений, а в управлении сложностью. Когда количество трансформаций растёт, ручное сопровождение SQL-скриптов или отдельных задач становится источником ошибок и несогласованности.

Управление трансформациями обеспечивает прозрачность: можно отследить, какие данные и по какой логике участвовали в формировании конкретного результата.

---

## Слайд 18. Эволюция трансформационной логики

Трансформационная логика не является статичной. По мере развития аналитической системы меняются источники, уточняются определения метрик, появляются новые требования к данным. Система трансформаций должна быть рассчитана на постоянные изменения.

Эволюция логики требует совместимости с историческими данными. Изменение правил не должно разрушать прошлые результаты или делать их несравнимыми без явного указания версии. Это достигается через контроль версий, изоляцию изменений и управляемые пересчёты.

Особую роль играет контракт между трансформациями и потребителями данных. Любое изменение логики должно быть осознанным и отслеживаемым, иначе аналитическая система теряет доверие.

Устойчивость к изменениям является главным показателем зрелости трансформационного слоя. Именно она отличает экспериментальный пайплайн от промышленной аналитической платформы.

## Слайд 1. Зачем Data Quality — как инженерная задача

**Data Quality** — это управляемое свойство данных, при котором результаты аналитики и моделей воспроизводимы, сравнимы во времени и не зависят от случайных дефектов источников или пайплайна. Без качества данные превращаются в набор файлов/таблиц, где любое число или график может быть артефактом загрузки, дедупликации, неверного join или частичной поставки. Главная инженерная проблема здесь не в «красоте данных», а в том, что дефекты масштабируются: одна ошибка на раннем слое размножается на агрегатах, витринах, отчётах и обучающих выборках.

Механика деградации качества обычно выглядит так: источник изменил схему/семантику → ingestion сохранил «как есть» → downstream-слой интерпретировал поле по старым правилам → агрегаты и метрики сдвинулись → BI показывает «аномалию», а ML обучается на искажённой истории. Эта цепочка опасна тем, что она часто не ломается технически: пайплайн может успешно завершиться, таблицы обновятся, но смысл станет неверным. В таких системах ключевой вопрос — не “упало или нет”, а “публиковать или блокировать”.

Ограничение: качество нельзя «прикрутить в конце». Проверки, выполненные только на витринах, обнаруживают дефект уже после того, как он распространился, и восстановление требует пересчётов. Второе ограничение — стоимость контроля: слишком строгие проверки блокируют публикацию из-за ложных срабатываний, слишком мягкие — пропускают дефекты в потребление.

Практически Data Quality организуют как набор измеряемых сигналов и правил принятия решений: что считать браком, что считать предупреждением, какие диапазоны допустимы, какие проверки обязательны перед публикацией в BI/ML. Это превращает качество в эксплуатационный контур: результаты проверок фиксируются, сравниваются по дням/запускам и используются для автоматических действий (стоп публикации, карантин, алерт, запуск backfill).

---

## Слайд 2. Термины и границы: data quality vs data validation vs observability

**Data validation** — проверка данных на соответствие формальным правилам: схема, типы, not null, уникальность ключей, диапазоны значений, корректность формата. Это “да/нет” контроль: набор утверждений, которые должны выполняться, иначе данные считаются неготовыми для следующего шага. Validation отвечает на вопрос: «эти данные допустимы по контракту и базовым ограничениям?».

**Data quality** шире: это не только выполнение правил, но и измерение состояния данных во времени. Качество включает статистические и поведенческие аспекты: стабильность распределений, полнота поставки, доля пропусков, частота дублей, согласованность агрегатов, задержка обновления. Quality отвечает на вопрос: «данные ведут себя как ожидается и дают корректный результат в потреблении?».

**Data observability** — слой наблюдаемости, который делает качество измеримым и управляемым: метрики, профили, линейдж, мониторинг отклонений, алерты, история изменений. Observability не заменяет validation: она помогает находить дефекты, которые не формализуются простыми правилами (например, плавный дрейф распределений или неожиданная смена кардинальности ключа). Она отвечает на вопрос: «что изменилось, где изменилось, почему это важно, как быстро это обнаружено?».

Ограничение validation: оно хорошо ловит “жёсткие” ошибки, но часто пропускает семантические сдвиги. Например, поле осталось числовым и не-null, но стало измеряться в другой единице — формально всё валидно, но смысл разрушен. Ограничение observability: она требует базовых линий (baseline), накопления истории и аккуратной настройки порогов, иначе алерты превращаются в шум.

Практически границы выглядят так: validation ставит минимальный барьер допуска данных между слоями, quality формирует показатели состояния, observability обеспечивает мониторинг и реакцию. Без такого разделения проверки расползаются по коду и становятся несогласованными: одна команда проверяет одно, другая — другое, и итоговая логика допуска становится непредсказуемой.

---

## Слайд 3. Измерения качества данных (DQ dimensions)

**Completeness** (полнота) — данные присутствуют в ожидаемом объёме и с ожидаемым покрытием. На уровне таблицы это контроль количества строк за период, на уровне колонок — доля NULL, на уровне сущностей — покрытие ключей (например, доля событий с user_id). Полнота редко равна 100% в реальности, поэтому она задаётся порогами и окнами (например, допускаем 0.5% пропусков по вторичным полям).

**Validity** (валидность) — значения соответствуют формату и допустимому домену. Примеры: статус в фиксированном наборе, сумма неотрицательная, дата не в будущем, идентификатор имеет ожидаемый паттерн. Валидация домена отличается от проверки типа: строка может быть строкой, но невалидной семантически.

**Uniqueness** (уникальность) — ключи, которые должны быть уникальны, действительно уникальны. Это не только “primary key”, но и любые детерминированные идентификаторы событий/операций/заказов, на которых строится дедупликация и инкрементальная загрузка. Нарушение уникальности почти всегда ведёт к раздуванию join и искажению агрегатов.

**Consistency** (согласованность) — разные представления одной и той же информации не противоречат друг другу. Это включает согласованность между слоями (RAW→Staging→Aggregated), согласованность справочников и фактов (все ключи факта присутствуют в измерении), согласованность сумм/агрегатов (балансы, сверки, контроль “не потеряли/не удвоили”). Consistency обычно проверяют через реконсиляцию и контроль кардинальности связей.

**Timeliness/Freshness** (своевременность/свежесть) — данные обновляются в ожидаемые сроки, а задержки измеримы. Freshness — это не “данные есть сегодня”, а измеренный лаг: время от события/поставки до публикации в слое потребления. Если лаг плавает, BI/ML получают несравнимые периоды и “ложные тренды” из-за неполных дней.

**Accuracy** (точность) — значения отражают реальность, но в аналитических пайплайнах это самая сложная метрика, потому что часто нет внешнего эталона. Accuracy обычно достигают косвенно: через сверки с контрольными системами, through-put проверки, баланс-контроли, и через согласованность разных источников. Там, где есть эталон, точность проверяют выборочно и автоматизируют как отдельный класс проверок.

Ограничение: не все измерения доступны одинаково. Completeness и validity относительно легко автоматизируются, consistency требует модели связей, accuracy часто требует внешних сверок. Практически их комбинируют: минимальные обязательные проверки + мониторинг распределений + периодические сверки.

---

## Слайд 4. Ошибки источников и ошибки пайплайна: разные причины — разные проверки

Ошибки источников возникают до попадания данных в платформу. Типовые классы: **schema drift** (добавили/переименовали поле, поменяли тип), частичные поставки (не все файлы/партиции доехали), повторы сообщений/файлов, некорректные значения из-за багов приложения, рассинхронизация справочников, нестабильные идентификаторы (ID переиспользуются или меняют формат). Эти дефекты часто выглядят как «валидные» записи, но ломают статистику, гранулярность или связи.

Ошибки пайплайна возникают внутри обработки и часто более коварны, потому что создаются самим преобразованием. Типовые классы: неверный join (many-to-many вместо many-to-one) → раздувание строк; инкрементальная загрузка без идемпотентности → дубликаты по периоду; неправильная перезапись партиций → “дыры” в истории; некорректная дедупликация (не тот ключ или неверный порядок по времени) → потеря событий; “тихий” каст типов → NULL вместо значений. Отдельный класс — ошибки публикации: данные посчитаны правильно, но опубликованы не полностью (часть партиций/файлов не записалась, транзакционность отсутствует).

Механика диагностирования различается. Для источников важны проверки поставки: наличие всех ожидаемых фрагментов, контроль размера/количества, контроль схемы, контроль доли ошибок парсинга. Для пайплайна важны реконсиляции и инварианты: контроль кардинальности, контроль сумм между слоями, контроль “не увеличили число строк неожиданно”, контроль “каждая публикация заменяет период целиком”.

Ограничение: источник может быть «правильным», а пайплайн сломан, и наоборот. Поэтому проверки должны уметь локализовать место дефекта: отличать “данные не пришли” от “данные пришли, но преобразование исказило”.

Практически это приводит к разделению сигналов: метрики поставки и схемы фиксируются на входе, метрики согласованности и инварианты — между слоями, метрики семантического поведения — на слоях потребления. Это снижает время поиска причины: “почему график поехал” превращается в проверку нескольких уровней.

---

## Слайд 5. Где живут проверки: RAW → Staging → Aggregated → витрины

На **RAW** уровне проверки направлены на факт поставки и техническую пригодность данных для дальнейшего чтения. Здесь уместны: наличие ожидаемых файлов/партиций, контроль размера и количества объектов, контроль доли ошибок чтения/парсинга, фиксация схемы как наблюдаемого артефакта (что реально приехало). RAW не должен «исправлять» данные, но обязан дать возможность доказать, что данные были получены и сохранены без потерь со стороны платформы.

На **Staging** уровне фокус смещается на структурную и базовую семантическую корректность. Здесь размещают: приведение типов, обязательные поля (not null для критичных ключей), валидность доменов (статусы, диапазоны), дедупликацию по заданным правилам, контроль уникальности технических идентификаторов, минимальные проверки ссылочной целостности там, где она уже возможна. Staging должен превратить “неустойчивую поставку” в стабильный набор, пригодный для join и агрегатов.

На **Aggregated** уровне появляются проверки согласованности и инварианты расчётов. Здесь уместны: реконсиляция счётчиков и сумм между входом и агрегатами, контроль покрытия периодов, контроль кардинальности группировок, контроль “нет неожиданных провалов/скачков” относительно истории, контроль корректности инкремента (перезаписали нужные партиции и только их). Aggregated слой часто является источником “истины” для расчётных показателей, поэтому на нём проверяют стабильность определений и воспроизводимость результатов.

На уровне **витрин** проверки ориентированы на готовность к потреблению: стабильная гранулярность, согласованные определения метрик, отсутствие разрывов по датам, предсказуемые NULL и категориальные значения, согласованность ключей для фильтрации и сегментации. Здесь также полезны “поведенческие” проверки: дашборды и модели чувствительны к резким сдвигам распределений, поэтому витрины проверяют как продукт, у которого есть SLA по свежести и набор ожидаемых статистик.

Ограничение: если проверки делают только на витринах, дефект уже распространился, а локализация причины становится дорогой. Если проверки делают только на RAW/Staging, семантические сдвиги могут пройти незамеченными и проявиться лишь в BI/ML. Практически контур качества строят как каскад: ранние слои ловят поставку/структуру, средние — согласованность, поздние — поведение и пригодность к потреблению, при этом результаты проверок хранятся отдельно как наблюдаемые данные (метрики качества по запускам и по периодам).

---
## Слайд 6. **Data Contracts** и ожидания потребителей данных

**Data contract** — формализованное соглашение о том, какие данные считаются корректными и стабильными для потребления. Контракт фиксирует не только схему (колонки и типы), но и более важные свойства: **гранулярность (grain)** набора данных, ключи и их уникальность, семантику полей, допустимые значения, частоту обновления и политику опоздавших данных. Без контракта любые проверки превращаются в набор несогласованных правил, а изменения источников начинают ломать BI/ML “тихо”, без явной точки принятия решения.

Контракт обычно разделяют на несколько уровней. **Schema contract** описывает обязательные колонки, типы, nullable/non-nullable и правила эволюции (что можно добавлять, что считается breaking change). **Semantic contract** описывает смысл ключевых полей и метрик: единицы измерения, интерпретацию статусов, правила дедупликации, что считается “валидной записью”. **Operational contract** фиксирует обновляемость: когда данные считаются “готовыми”, какой лаг допустим, какие периоды могут пересчитываться задним числом.

Проблема контрактов в том, что их легко написать и сложно поддерживать. Семантика меняется чаще схемы: поле может остаться числом, но перестать быть “суммой” и стать “суммой без скидок”, и формальные проверки это не поймают. Вторая проблема — множество потребителей: один и тот же набор данных может использоваться для разных задач, и изменения ради одной задачи ломают другие, если нет версионирования и обратной совместимости.

Практически контракт хранят как код: в репозитории, рядом с трансформациями, с версионированием и review. Минимальный набор, который контракт должен фиксировать для каждой таблицы/набора: grain, primary key (или ключ дедупликации), список критичных колонок, политика null, допустимые домены значений, политика обновления (append/overwrite по партиции), окно поздних данных и правила backfill. Проверки контракта должны выполняться автоматически на границе публикации слоя: если контракт нарушен, данные не переходят дальше.

Контракт связывает качество с архитектурой слоёв: RAW фиксирует факт поставки, Staging фиксирует техническую совместимость, Aggregated фиксирует согласованность расчётов, витрины фиксируют стабильное потребление. Контракт задаёт точку, в которой “данные считаются продуктом” и не могут изменяться незаметно.

---

## Слайд 7. Схемная валидация и контроль изменений: **schema validation** и **schema drift**

**Schema validation** — проверка того, что структура данных соответствует ожидаемой: набор колонок, типы, nullable, форматы, совместимость. **Schema drift** — неконтролируемое изменение схемы источником или промежуточным шагом: добавление/удаление/переименование колонок, смена типа, изменение вложенной структуры. Drift опасен тем, что может не приводить к падению пайплайна: данные читаются, но часть колонок становится null или интерпретируется неверно.

Механика контроля схемы обычно двухступенчатая. На входе фиксируется “факт схемы” поставки: что реально пришло в RAW (список колонок, их типы и дополнительные признаки). На переходе в Staging выполняется сопоставление с ожидаемой схемой: обязательные колонки должны быть, типы должны быть совместимы, новые колонки допускаются только по правилам эволюции. Смена типа почти всегда breaking change, потому что ломает агрегаты и join, даже если Spark/SQL умеет выполнить неявный cast.

Основные режимы политики схемы:

- additive change: добавили новую колонку — допускается, если потребители не зависят от неё как от обязательной;
    
- breaking removal/rename: удалили или переименовали — блокирует публикацию, потому что downstream получает неполный набор полей;
    
- type widening: безопасное расширение (например, int → bigint) может допускаться, но должно быть явно отражено в контракте;
    
- type change: string ↔ numeric или изменение семантики времени почти всегда требует отдельного релиза и backfill.
    

Проблемы и ограничения: формальная схема не ловит “семантический drift”, когда тип тот же, но смысл поменялся. Вторая проблема — вложенные структуры и массивы: их изменения часто маскируются, а downstream-логика начинает выдавать null на отдельных ветках JSON. Третья — постепенная эволюция: источник может выкатывать изменения не атомарно, и несколько дней данные приходят “в смеси” двух схем.

Практически схемную валидацию делают максимально ранней, но решения — максимально консервативными. Добавление колонок допускают, удаление/смену типов блокируют или отправляют в карантин, чтобы не ломать downstream. Простой технический пример проверки в Spark — чтение с ожидаемой схемой (если несовместимо, чтение/касты явно сигнализируют проблему) и сравнение фактической схемы с эталоном; в SQL-слое — проверка `system.columns` относительно ожидаемого набора и типов для публикуемых таблиц.

Схемная валидация связывает ingestion и трансформации: если drift ловится на входе, можно остановить публикацию до того, как дефект распространится на агрегаты и витрины. Это снижает стоимость исправления, потому что не требуется пересчитывать большой диапазон данных.

---

## Слайд 8. Проверки ключей и связей: **uniqueness**, **not null**, **referential integrity**, кардинальность join

Ключевые проверки качества в аналитике строятся вокруг того, что данные должны быть “соединяемыми” и “считаемыми”. Для этого фиксируются ключи: идентификаторы событий/записей, ключи сущностей, ключи дедупликации, а также правила связей между наборами данных. Нарушение ключей приводит к двум типам дефектов: скрытые потери (строки выпадают из join) и скрытое раздувание (many-to-many join взрывает объём и агрегаты).

**Uniqueness** проверяет, что ключ, объявленный уникальным, действительно уникален в пределах ожидаемой гранулярности. В аналитике уникальность часто “партиционная”: уникален по `(id, date)` или по `(event_id)` в рамках периода. **Not null** для ключей означает, что записи без ключа либо должны быть отбракованы/карантинизированы, либо должны получать технический ключ по чётким правилам (иначе downstream превращается в неопределённость).

**Referential integrity** в аналитических системах редко реализуется как жёсткие FK-constraint в хранилище, но проверяется аналитическими тестами. Базовая проверка — анти-join: доля фактовых строк, для которых нет ключа в измерении, должна быть ниже порога (или равна нулю для критичных связей). Пример SQL-проверки “висячих ключей”:

```sql
SELECT count() AS orphan_cnt
FROM fact f
LEFT ANTI JOIN dim d ON f.key = d.key;
```

Кардинальность связей — отдельный класс проверок. Если ожидается many-to-one (факт → измерение), измерение должно иметь уникальный ключ. Если в измерении появились дубликаты ключа, join станет many-to-many и раздует строки, иногда на порядки. Это проверяют как “uniqueness dim key” и как контроль коэффициента раздувания: `rows_after_join / rows_before_join` не должен превышать допустимое значение.

Практические аспекты: ключи и связи проверяются на Staging/Aggregated слоях до публикации в потребление. Для больших объёмов используют выборочные проверки по партициям, контроль топ-ключей по частоте, и агрегированные метрики (сколько дублей, сколько orphan). Результаты проверок фиксируются в отдельной таблице качества, чтобы видеть динамику: рост orphan-ключей часто сигнализирует о рассинхронизации справочников или об изменении логики формирования ключа.

Эти проверки связывают качество данных с вычислительной стоимостью: корректные ключи уменьшают риск тяжёлых shuffle/join и делают агрегаты стабильными. Нарушения ключей почти всегда проявляются либо в аномальных метриках, либо в деградации производительности, поэтому ключевые проверки — базовый слой защиты платформы.

---

## Слайд 9. Реконсиляция и баланс-контроли: **reconciliation** между слоями

**Reconciliation** — проверка того, что при переходе между слоями не произошло нежелательных потерь, дублирования или смещения сумм. Это более сильный класс проверок, чем “колонка not null”: он проверяет инварианты на уровне набора данных и метрик. Реконсиляция особенно важна там, где трансформации могут менять кардинальность: дедупликация, join со справочниками, агрегирование, фильтрация “невалидных” записей.

Базовые сигналы реконсиляции:

- контроль числа строк по периоду и по ключевым срезам (например, по типам событий);
    
- контроль сумм числовых мер (revenue, amount) по периоду и по срезам;
    
- контроль distinct-ключей (сколько уникальных `event_id`, `user_id`), если это инвариант слоя;
    
- контроль “коэффициента трансформации”: отношение входа к выходу должно лежать в ожидаемом диапазоне.
    

Реконсиляция почти всегда делается по партициям, потому что это снижает стоимость и упрощает локализацию дефекта. Пример контрольного сравнения сумм между Staging и Aggregated на дневной партиции:

```sql
SELECT
  s.event_date,
  s.sum_amount AS staging_sum,
  a.sum_amount AS agg_sum,
  (a.sum_amount - s.sum_amount) AS diff
FROM
  (SELECT event_date, sum(amount) AS sum_amount FROM staging GROUP BY event_date) s
JOIN
  (SELECT event_date, sum(amount) AS sum_amount FROM aggregated GROUP BY event_date) a
USING (event_date);
```

Проблемы и ограничения: некоторые трансформации легально меняют суммы/строки (например, фильтрация брака или корректировки). Поэтому реконсиляция требует “учёта причин”: либо отдельной метрики “сколько отбраковано и почему”, либо отдельного слоя/таблицы quarantine, чтобы разница была объяснимой. Второе ограничение — допустимые отклонения: в реальности бывает частичная поставка или поздние данные, поэтому вместо “строго равно” используют пороги и окна готовности.

Практически реконсиляция оформляется как набор контролей, которые дают численные результаты и статус (pass/warn/fail). Эти результаты сохраняются как данные: таблица `dq_checks` с датой партиции, названием проверки, измеренным значением, порогом и статусом. Такой подход позволяет не только блокировать публикацию при провале, но и анализировать тренды качества: постепенный рост расхождений часто выявляет проблему раньше, чем её заметят по графику в BI.

Реконсиляция связывает качество с эксплуатацией: именно через неё можно доказать, что трансформация не потеряла строки и не исказила суммы. Это фундамент для доверия к данным, потому что проверки формируют “цепочку доказательств” между слоями.

---

## Слайд 10. Аномалии и мониторинг качества как временных рядов

Многие дефекты данных не нарушают формальные правила. Таблица может иметь правильную схему, ключи могут быть не-null, тесты uniqueness могут проходить, но распределения и агрегаты начинают вести себя иначе из-за дефектов поставки или изменения поведения источника. Для таких ситуаций используют мониторинг аномалий: качество измеряется как временной ряд метрик, и отклонения от ожидаемого поведения сигнализируют проблему.

Типовые метрики наблюдаемости качества:

- объём: число строк/событий по партиции, доля ошибок парсинга, доля quarantine;
    
- заполненность: доля NULL по критичным колонкам, доля пустых строк/значений;
    
- распределения: доли категорий (статусы/типы), квантили числовых полей (p50/p95/p99);
    
- кардинальность: число уникальных ключей, среднее число событий на ключ;
    
- freshness: задержка обновления и время последней успешной публикации.
    

Обнаружение аномалий обычно строится на базовой линии (baseline) и правилах отклонения. Самый распространённый промышленный подход — сравнение текущего значения с историческим окном: rolling mean/median и допустимое отклонение, иногда с учётом дня недели. Пример минимальной логики в Python для сигнала по объёму (идея, не “модель”):

```python
x = current_rows
mu = history_rows.mean()
sigma = history_rows.std()
alert = abs(x - mu) > 3 * sigma
```

Проблемы и ограничения: сезонность и календарные эффекты дают много ложных срабатываний, если сравнивать “среднее за 7 дней” без учёта дня недели. Вторая проблема — дрейф: система может плавно перейти на новый уровень объёма, и простые пороги начинают постоянно “краснеть”. Поэтому мониторинг строят как комбинацию: сравнение с прошлой неделей (same weekday), контроль относительного изменения (rate-of-change), контроль диапазона, контроль стабильности долей категорий.

Практические аспекты: аномалии не должны блокировать пайплайн автоматически так же жёстко, как validation. Обычно это уровень warn/fail по порогам: критические отклонения (например, падение объёма на 80%) блокируют публикацию, умеренные — создают алерт и пометку о снижении доверия. Метрики аномалий также сохраняются в отдельные таблицы наблюдаемости, чтобы видеть, какие сигналы срабатывают чаще всего и где нужно улучшать upstream или уточнять правила.

Этот слой связывает data validation с реальной эксплуатацией BI/ML: он ловит дефекты, которые формальные тесты пропускают, и уменьшает время до обнаружения. Чем раньше фиксируется отклонение поведения данных, тем меньше диапазон backfill и тем дешевле восстановление корректного состояния.

---
## Слайд 11. Политики обработки брака: **fail / soft-fail / quarantine**, уровни строгости

Качество данных в эксплуатации — это не только проверки, но и правило, что делать при провале. Без формализованной политики пайплайн либо бесконечно “краснеет” и блокирует всё, либо пропускает дефекты в BI/ML. Политика строится вокруг трёх режимов реакции: **fail**, **soft-fail**, **quarantine**. Они применяются не глобально, а по классам проверок и по слоям.

**Fail** означает блокировку публикации результата в следующий слой или в потребление. Его используют только для проверок, нарушение которых делает данные непригодными технически или логически: несовместимая схема, сломанные ключи, дубли на уникальном идентификаторе, нулевая поставка вместо ожидаемой, неконсистентность, ведущая к раздуванию join. Fail должен быть детерминированным: одна и та же входная ситуация всегда приводит к одному решению, иначе пайплайн становится нестабильным.

**Soft-fail** означает, что публикация продолжается, но фиксируется “пониженное доверие”: метка качества, алерт, запись результата проверки с уровнем severity, возможно — скрытие данных от части потребителей (например, только internal). Soft-fail применяют к аномалиям, где высока вероятность ложного срабатывания: умеренные отклонения объёма, рост доли NULL по некритичным полям, изменение распределений в пределах допуска. Этот режим полезен, когда важнее свежесть, чем идеальная чистота, но при этом нельзя терять наблюдаемость.

**Quarantine** — изоляция части данных, которые нарушают правила, с сохранением их как отдельного артефакта. В отличие от “просто отфильтровать”, quarantine позволяет:

- не терять проблемные строки (можно разбирать и чинить правила);
    
- объяснять расхождения в реконсиляции (часть данных ушла в брак);
    
- строить отчётность по причинам дефектов (какой тип ошибок доминирует).
    

Обычно quarantine реализуют как отдельную таблицу/датасет с обязательным набором технических полей: `rule_id`, `rule_version`, `error_type`, `error_payload` (что именно нарушено), `batch_id`, `load_ts`, а также копия исходной строки или её ключей. Пример схемы записи результата проверки (идея):

```sql
INSERT INTO dq_quarantine
SELECT
  '{{ batch_id }}' AS batch_id,
  now() AS load_ts,
  'rule_not_null_user_id' AS rule_id,
  1 AS rule_version,
  toJSONString(*) AS row_payload
FROM staging_events
WHERE user_id IS NULL;
```

Уровни строгости обычно задают как минимум 3 градации: **critical / major / minor**. Critical блокирует публикацию или переводит слой в “не готов”; major может переводить конкретные партиции в quarantine/не публиковать; minor фиксируется как предупреждение. Это связывается с матрицей “слой × правило”: например, schema drift в Staging — critical, а дрейф распределений на витрине — major/minor.

---

## Слайд 12. **Freshness** и SLA/SLO: когда данные считаются “готовыми”

Freshness — измеряемая задержка данных относительно ожиданий потребителей. В аналитике важно различать:

- **data freshness**: насколько свежи опубликованные данные (время последней успешной публикации);
    
- **data latency/lag**: задержка между временем события и временем появления события/агрегата в слое потребления;
    
- **pipeline freshness**: регулярность и длительность выполнения пайплайна (run duration, start delay).
    

SLA/SLO для данных обычно формулируют вокруг двух параметров: **окно готовности** и **максимально допустимый лаг**. Окно готовности задаёт момент, когда период можно считать “закрытым” для BI/ML. Например: “дневная партиция считается готовой в 06:00 следующего дня” или “часовая — через 20 минут после конца часа”. При этом в реальности почти всегда есть **late arrivals** — события, которые приходят позже. Тогда нужен явный контракт: либо “мы пересчитываем последние N дней”, либо “поздние данные попадают в отдельный контур корректировок”.

Технически freshness измеряют метриками, которые можно вычислять автоматически:

- `max(event_time)` в опубликованном наборе;
    
- `max(ingestion_time)` или `load_ts` в слое;
    
- наличие ожидаемых партиций/файлов;
    
- доля “незаполненных” партиций за период.
    

Пример метрики lag для партиции:

```sql
SELECT
  event_date,
  dateDiff('minute', max(event_time), now()) AS event_lag_min,
  dateDiff('minute', max(load_ts), now()) AS publish_lag_min
FROM mart_events
WHERE event_date >= today() - 7
GROUP BY event_date;
```

Проблема freshness — неполные периоды. Если дневная партиция публикуется до того, как приехали все данные, BI видит “падение метрик”, которое исчезнет после дозагрузки. Это решают политикой публикации:

- либо не публиковать период до закрытия окна готовности;
    
- либо публиковать, но маркировать период как **partial**, чтобы потребление умело фильтровать;
    
- либо публиковать предварительно, но гарантировать пересчёт и замену партиции в пределах окна late arrivals.
    

SLA/SLO связывают с алертами и правилами допуска: например, “если publish_lag > X минут — блокируем обновление витрины и поднимаем алерт”, “если период partial — скрываем из внешнего BI”. Это делает свежесть не “ощущением”, а контролируемым свойством данных.

---

## Слайд 13. Подготовка данных для BI: стабильность, зерно, семантика, предсказуемость

Данные для BI должны быть стабильными по структуре и смыслу, потому что BI-слой строится на повторяемых запросах и ожидании сравнимости периодов. В подготовке для BI ключевое — зафиксировать **зерно (grain)** и обеспечить семантическую согласованность: одна строка должна означать одну и ту же вещь всегда, а агрегаты должны быть сравнимы между датами.

**Grain** — это определение “что такое строка”. Примеры: “одна строка = один user×day”, “одна строка = один order”, “одна строка = один event”. BI-ошибки часто начинаются, когда grain меняется незаметно (например, после join с неуникальным измерением строка становится “user×day×category”). Поэтому для BI наборов данных важно иметь проверки на:

- уникальность ключа, который задаёт grain;
    
- кардинальность join (dimension key уникален);
    
- стабильность числа строк на период в ожидаемом диапазоне.
    

BI плохо переносит “плавающие” NULL и категориальные домены. Для подготовки данных нужно сделать поведение полей предсказуемым:

- критичные фильтровые поля должны быть либо заполнены, либо иметь явную категорию `unknown`;
    
- категориальные поля должны иметь контролируемый словарь (или хотя бы мониторинг появления новых категорий);
    
- числовые метрики должны иметь ясные правила: где 0, а где NULL, и почему.
    

В BI слой часто вводят “публикационный” контракт: какие колонки гарантируются, какие могут появляться, как называются поля, как трактуются статусы, какие периоды пересчитываются. Это критично для дашбордов: если определение метрики изменилось, BI должен либо получить новую версию метрики/поля, либо метку версии данных.

Минимальная подготовка BI-наборов обычно включает:

- контроль grain и ключей;
    
- контроль полноты периодов (нет дыр по датам);
    
- контроль базовых распределений категорий;
    
- реконсиляцию сумм/счётчиков с upstream слоями;
    
- маркировку качества/partial состояния на уровне партиции.
    

Это не “добавочные проверки”, а условия, чтобы BI-результаты не превращались в набор несогласованных графиков.

---

## Слайд 14. Подготовка данных для ML: **train/serve consistency**, snapshots, защита от утечек

Подготовка данных для ML отличается тем, что модель обучается на истории, а применяется на будущих данных. Ключевой риск — несоответствие между тем, как признаки сформированы на обучении, и тем, как они формируются в проде (**train/serve skew**). Второй риск — **data leakage**: в признаки попадает информация, которая недоступна на момент предсказания, из-за чего качество на обучении выглядит высоким, а в реальности разваливается.

Основа ML-подготовки — **snapshot-логика**: признаки должны строиться так, как если бы система “заморозила” состояние на момент T и посчитала признаки, используя только данные ≤ T. Это означает:

- все окна и лаги считаются относительно cut-off времени;
    
- поздние данные не должны “поправлять” прошлые признаки, если модель в проде так не работает;
    
- целевая переменная (label) должна быть определена строго после cut-off, иначе возникает утечка.
    

Типовые проверки для предотвращения leakage:

- каждая feature-таблица должна иметь явный `feature_time` (момент, на который признаки рассчитаны) и `data_max_time` (максимальное время использованных событий);
    
- проверка `data_max_time <= feature_time` для всех строк;
    
- проверка, что join’ы не подтягивают будущее состояние (например, справочник, который обновляется со временем, должен быть versioned или привязан к effective_date).
    

Пример контрольного условия (концептуально):

```sql
SELECT count()
FROM ml_features
WHERE data_max_time > feature_time;
```

ML также чувствителен к дрейфу признаков: распределения фичей меняются, появляются новые категории, растёт доля пропусков. Поэтому подготовка для ML включает те же базовые метрики качества, что и для BI, но на уровне признаков:

- доля NULL/NaN по фичам;
    
- квантили числовых фичей;
    
- частоты категорий и доля “unknown”;
    
- стабильность количества объектов (сколько строк для обучения/скоринга на период).
    

Отдельная практическая часть — воспроизводимость выборок:

- фиксированные правила фильтрации (кто попадает в датасет);
    
- детерминированные ключи и дедуп;
    
- сохранение версии трансформаций и параметров окна (N дней, правила агрегирования);
    
- хранение результатов DQ/validation рядом с датасетом, чтобы можно было объяснить, почему модель обучалась на “таком” наборе.
    

---

## Слайд 15. Инструменты и реализация в стеке: SQL/Spark/Python, dbt tests, таблицы качества и алерты

Контур качества реализуют как отдельный слой артефактов, а не как “пара if-ов в коде”. Минимальный промышленный набор включает:

- набор правил (validation checks);
    
- таблицу результатов проверок (по партиции/запуску);
    
- таблицу quarantine (для брака);
    
- метрики observability (объёмы, null-rate, распределения, lag);
    
- механизм принятия решения (gate) о публикации.
    

В SQL-слое (ClickHouse/аналог) удобно делать:

- проверки ключей/уникальности (агрегации по ключу с порогами);
    
- проверки доменов значений (countIf по условиям);
    
- реконсиляции сумм и счётчиков между слоями;
    
- freshness/lag метрики по `load_ts` и `event_time`.
    

В Spark/Python удобно делать:

- профилирование распределений на больших объёмах (квантили, heavy hitters);
    
- проверку сложных правил, которые неудобно выражать в SQL;
    
- подготовку отчётов и запись результатов в dq-таблицы;
    
- вычисление baseline и детект аномалий (rolling статистики).
    

dbt tests полезен там, где трансформации и публикация витрин реализованы как SQL-модели:

- `unique`, `not_null`, `accepted_values`, `relationships`;
    
- кастомные тесты на реконсиляцию и кардинальность;
    
- привязка тестов к моделям и зависимостям, чтобы “готовность” слоя была формализована.
    

Стандартный паттерн хранения результатов:

- `dq_check_results`: `check_time`, `batch_id`, `layer`, `table`, `partition`, `check_name`, `metric_value`, `threshold`, `status`, `severity`, `details`;
    
- `dq_quarantine`: `batch_id`, `rule_id`, `rule_version`, `partition`, `row_keys`, `row_payload`, `created_at`.
    

Gate (решение о публикации) строится на статусах `critical` проверок: если есть fail по critical — следующий слой не обновляется/партиция не публикуется. Если есть major/minor — публикуется с метками, но создаётся алерт и запись об отклонении.

Алерты подключаются к событиям “публикация заблокирована”, “freshness нарушена”, “аномалия метрики качества”: Telegram/email/Slack — это транспорт, но полезность даёт структура сообщения: какая проверка, какая партиция, текущее значение, порог, ссылка на результаты и на lineage. Это превращает качество в эксплуатационную практику, а не в разрозненные проверки, которые никто не смотрит.

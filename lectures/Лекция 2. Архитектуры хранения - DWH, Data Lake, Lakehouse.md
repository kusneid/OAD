### 1) Проблема хранения аналитических данных

В аналитике важны три вещи одновременно:

1. **История**  
    Продуктовая база обычно хранит “как сейчас”. Аналитика почти всегда требует “как было”:
- что происходило по дням/неделям,
    
- как менялась выручка,
    
- как изменялся пользователь (статус, тариф, сегмент),
    
- что было до и после фичи/акции.
    

История быстро раздувает объёмы. Даже если “сейчас” у вас 1 млн пользователей, “история по дням” превращает это в десятки/сотни миллионов строк.

2. **Скорость ответов на вопросы**  
    В аналитике вопросы часто такие:
    

- “покажи DAU по дням за год по всем городам”
    
- “сравни выручку по 10 сегментам + добавь разрез по каналу”  
    Это означает большие сканы, группировки, сортировки, фильтры, джойны — то, что не похоже на типичные запросы приложения.
    

3. **Дешевизна и масштабирование**  
    У аналитики есть неприятная особенность: вы не можете заранее знать, какие вопросы завтра задаст бизнес.  
    Сегодня считают DAU, завтра захотят “DAU по устройствам + по источнику + по типу тарифа + только новички”.  
    Поэтому хранение должно быть:
    
- либо очень гибким (и тогда нужно уметь дорого считать),
    
- либо очень быстрым (и тогда нужно заранее готовить витрины).
    

И вот тут рождается главный конфликт архитектур хранения:

- **Дёшево хранить всё и долго** (файлы/объектное хранилище)  
    vs
- **Быстро отвечать на BI-запросы** (аналитические БД / DWH)

Второй важный конфликт:

- **данные сырые, разные, меняются**  
    vs
- **аналитика хочет стабильные схемы и одинаковые определения метрик**

Эти два конфликта и объясняют, почему в реальной архитектуре редко бывает “одна база и всё”.

Что студенты должны вынести из этого пункта:  
хранилище в аналитике выбирают не по “мне нравится эта БД”, а по тому, какую часть конфликта вы решаете: дешевое хранение истории или быстрые ответы на запросы.

---

### 2) Почему одной СУБД недостаточно

Тут нужно аккуратно “сломать” привычную картинку студентов: “у нас же есть PostgreSQL/MySQL — почему бы не считать там?”

Раздели ответ на три группы причин.

**(A) Разные типы нагрузок: OLTP vs OLAP**

- OLTP — много коротких запросов и транзакций: добавить заказ, обновить статус, получить пользователя по id. примеры - PostgreSQL, MySQL, SQLite etc
    
- OLAP — длинные запросы, которые читают много строк: GROUP BY, оконные функции, тяжёлые JOIN, сканирование по времени. примеры - ClickHouse, GreenPlum, DuckDB
    

Если вы запускаете OLAP на OLTP:

- аналитика становится медленной,
    
- прод начинает тормозить,
    
- вы начинаете “городить”: реплики, ночные выгрузки, отдельные таблицы, запреты на запросы в рабочее время.
    

**(B) Стоимость хранения и вычислений**  
В OLTP хранить “всё сырое и навсегда” обычно дорого/неудобно:

- индексы,
    
- вакуумы,
    
- рост таблиц,
    
- бэкапы,
    
- нагрузка на диск и на обслуживание.
    

Плюс сырые данные — это часто не аккуратные таблицы, а:

- логи,
    
- события,
    
- полуструктурированный JSON,
    
- файлы внешних источников.
    

**(C) Воспроизводимость и пересчёты**  
В аналитике постоянно меняются определения:

- “DAU считать по логинам или по событиям?”
    
- “выручку считать по оплатам или по начислениям?”
    
- “пользователь активный — это 1 событие или 3?”
    

Если у вас нет нормально организованного хранения сырых данных и слоёв обработки — вы не сможете корректно пересчитать историю. А пересчёт истории в аналитике — нормальная жизнь.

Ключевая мысль для студентов:  
одна СУБД хороша для одной цели. Как только появляются разные цели (операционка + аналитика + хранение сырья), появляется архитектура из нескольких компонентов.

---

### 3) Классический DWH: идея и назначение

Здесь важно дать “правильное” определение DWH без лишней теории.

DWH — это хранилище, в котором данные:

- **структурированы**,
    
- **согласованы**,
    
- **с едиными определениями**,
    
- **удобны для аналитических запросов и BI**,
    
- **служат источником правды для метрик**.
    

Главная роль DWH — не “хранить данные”, а **стандартизировать смысл** данных.

Что обычно делает DWH:

- объединяет источники,
    
- создаёт единую модель сущностей: user, payment, ride, order, product, city, channel,
    
- фиксирует правила: что такое DAU, revenue, conversion, retention.
    

Важно проговорить отличие от “витрин”:

- DWH — это “база для аналитики”, на которой можно строить много разных задач
    
- витрины — “готовые ответы” под конкретный набор вопросов (часто денормализованные, широкие)
    

На этом же месте можно упомянуть, что DWH часто строят по понятным моделям:

- факты/измерения (звезда/снежинка)  
    но **не углубляйся**, просто скажи:  
    “это делается, чтобы аналитикам было проще писать запросы”.
    

И ещё важная мысль:  
DWH не обязательно “одна огромная монолитная база”. DWH — это роль в архитектуре: место, где смысл данных фиксируется и становится стабильным.

---

### 5) Ограничения DWH при работе с сырыми данными

Этот пункт нужен, чтобы естественно привести студентов к Data Lake.

Проблемы DWH, если пытаться засунуть туда всё подряд:

1. **Сырьё полуструктурированное**  
    JSON-события, логи, нестабильные схемы.  
    В DWH вы начинаете:
    

- постоянно менять таблицы,
    
- ловить ошибки на загрузке,
    
- поддерживать огромные “сырые” таблицы, которые никто не хочет трогать.
    

2. **Схемы источников меняются**  
    В реальности это постоянно:  
    добавили колонку, переименовали поле, изменили тип.
    

Если вы сразу грузите в DWH без буфера и правил, то:

- пайплайн ломается,
    
- витрины начинают расходиться,
    
- аналитики теряют доверие.
    

3. **Цена хранения “всего навсегда”**  
    В DWH хранить огромные объёмы сырца обычно дороже и менее удобно, чем в объектном хранилище.  
    Особенно если вы храните не только “полезные” поля, а полные payload’ы событий.
    
4. **Пересчёты и версии**  
    Если вы меняете логику обработки, вам нужна возможность вернуться к исходным данным и пересчитать.  
    С DWH это часто тяжелее и дороже, чем если у вас есть отдельный слой хранения сырья.
    

Вывод этого пункта:  
DWH отлично подходит как “слой смысла и быстрых запросов”, но плохо как “универсальный склад всего сырья”.

---

### 6) Появление Data Lake как ответ на ограничения DWH

Теперь логичный переход:  
“Окей, DWH не хочет быть свалкой сырья. Где тогда хранить всё?”

Data Lake — это архитектурная идея:

- хранить данные “как файлы” в масштабируемом и дешёвом хранилище,
    
- принимать данные из любых источников и в любых форматах,
    
- сохранять историю,
    
- не требовать жёсткой модели на входе.
    

Важное уточнение:  
Lake — это **про хранение и гибкость**, а не про “быстрые BI отчеты”.

Почему Lake появился:

- нужно хранить **очень много данных дешево**
    
- нужно хранить **разные типы данных**
    
- нужно хранить **сырые данные для пересчётов**
    
- нужно разгрузить DWH от грязных данных
    

Если вспоминать слои из прошлой лекции:

- RAW и часто Staging живут в Lake
    
- витрины и BI — чаще в DWH/OLAP
    

Хороший “приземляющий” пример, который студенты понимают:

- В приложении: данные в Postgres нужны, чтобы приложение работало.
    
- В Lake: те же данные складывают “для истории и аналитики”, потому что бизнес потом захочет считать то, что не было предусмотрено изначально.

### 7 Архитектура Data Lake (S3, HDFS)

Data Lake — это не “одна технология”, а **способ хранить данные**: как файлы в распределённом хранилище, чтобы было **дёшево, много, гибко**. В классическом понимании Lake состоит из:

- физического хранилища (где лежат файлы),
    
- структуры каталогов (как всё организовано),
    
- правил именования и партиционирования,
    
- метаданных (чтобы понимать, что за таблица и какие у неё колонки),
    
- движков обработки (которые эти файлы читают и пишут).
    

**S3 (и S3-compatible)**  
Современный “Lake по умолчанию” — это объектное хранилище в стиле S3.
Ключевые особенности:

- нет “папок” как на диске, есть ключи объектов, но визуально это похоже на каталоги;
    
- вы храните файлы любого объёма и типа;
    
- вы масштабируетесь не “добавлением диска”, а тем, что хранилище изначально рассчитано на огромные объёмы.
    

В условиях текущих ограничений в РФ лучше говорить шире: **S3-совместимое хранилище** — это может быть:

- облачное объектное хранилище (например, в локальных облаках, таких как Yandex Cloud, Vk cloud),
    
- или self-host (MinIO) — очень частая практика.
    

Ключевой смысл: S3-подобное хранилище идеально подходит как место, куда вы складываете:

- сырьё (raw),
    
- промежуточные слои (staging),
    
- историю и архивы,  
    потому что это дешево и масштабируется.
    

**HDFS (исторически и концептуально)**  
HDFS полезно объяснить как “классический big data storage”, на котором выросла экосистема Hadoop. Идеи HDFS:

- данные режутся на блоки,
    
- блоки распределяются по узлам кластера,
    
- чтение/обработка делаются параллельно.
    

Но важно сразу зафиксировать:

- сегодня в практике часто вместо HDFS приветствуются S3-подобные хранилища, тк они гораздо более практичные, их гораздо легче развернуть
    
- HDFS важен как концепция “распределённого хранения”, но Lake в современном курсе проще показывать через S3/MinIO.
    
- Но при этом стоит учесть что в Российских бигтехах(как минимум - Сбер, Т-банк, ВК, МТС, Мегафон) Apache Hadoop все ещё является стандартом по части DataLake хранилищ

**Почему это вообще называется lake**  
Потому что вы можете складывать туда всё:

- таблицы из БД,
    
- логи,
    
- события,
    
- внешние файлы,
    
- JSON/XML,
    
- выгрузки раз в день/час,  
    и не обязаны на входе иметь идеальную модель данных, как в DWH.
    

Главная мысль для студентов:  
Data Lake — это “фундамент хранения”, но не “место, куда ходит BI”.

### 8. Типы данных в Data Lake (RAW, Staging)

Чтобы Lake не превратился в мусорку, в нём почти всегда есть логические слои - RAW и Staging как мы обсуждали на первой лекции

**RAW в Lake**
RAW — это данные:

- максимально близко к источнику,
    
- с минимальными изменениями,
    
- обычно с техническими полями вроде `ingestion_ts`, `source`, `load_dt`.
    

Что реально делают в RAW:

- могут только “упаковать” событие в файл,
    
- добавить дату загрузки,
    
- иногда — разложить по партициям (по дате).
    

RAW нужен, чтобы:

- ничего не потерять,
    
- иметь возможность пересчитать любую витрину,
    
- уметь отладить ошибку.
    

**Staging в Lake**  
Staging — это “технически корректная версия данных”:

- типы приведены,
    
- дата/время нормализованы,
    
- дубликаты удалены,
    
- структура стала табличной и удобной для дальнейших JOIN’ов.
    

Это можно объяснить так:  
RAW — это “архив входящих”, Staging — это “рабочий полуфабрикат”.

**Как слои выглядят в S3 на практике**  
Очень важно дать студентам **ощущение структуры**, иначе они не поймут, что именно строится.

Типовой паттерн хранения:

- `lake/raw/<source>/<entity>/ingestion_date=YYYY-MM-DD/part-....parquet`
    
- `lake/staging/<domain>/<entity>/dt=YYYY-MM-DD/part-....parquet`
    

Где:

- `<source>` — откуда пришло (app, payments_db, crm_api),
    
- `<entity>` — что это (rides, users, payments),
    
- `ingestion_date` / `dt` — партиция по дате.
    

Зачем партиции:

- чтобы не читать весь год данных, когда нужна неделя,
    
- чтобы Spark/движок мог “отрезать лишнее” на чтении,
    
- чтобы пересчёты были дешевле.
    

Отдельно можно проговорить проблему: “а что если в событии есть `event_time`, а загрузили мы позже?”  
Тогда вы разделяете:

- **partition by load date** (как пришло),
    
- и отдельно храните `event_time` внутри данных, чтобы потом строить аналитику по фактическому времени события.
    

**Граница ответственности слоёв**  
Важная мысль: Staging всё ещё не про бизнес-метрики.  
Staging не отвечает на “какая выручка?”, он отвечает на “данные теперь честные и пригодные для расчётов”.

---

### 9. Проблемы классического Data Lake

Теперь логика: “мы сделали lake, но почему этого недостаточно?”  
Вот здесь и появляется боль, из-за которой придумали lakehouse.

**Проблема 1: Data Swamp (болото данных)**  
Если в lake нет строгих правил, случается:

- десятки версий одного и того же датасета,
    
- непонятно, какая актуальная,
    
- кто-то поменял формат/колонку — и всё сломалось,
    
- новые люди не понимают, где что лежит.
    

Студентам это легко показать на бытовом уровне:  
“у вас на компе папка `Downloads` — это маленький data swamp”.

**Проблема 2: Отсутствие единого “табличного контракта”**  
В базе данных таблица — это объект: у неё есть схема, ограничения, понятные операции.  
В классическом lake у вас просто набор файлов. Отсюда проблемы:

- schema drift: колонки “плывут” от файла к файлу,
    
- сложно гарантировать, что “таблица” сегодня такая же, как вчера,
    
- сложно объяснить BI/аналитикам “куда писать запрос”.
    

**Проблема 3: Мелкие файлы и производительность**  
В lake часто появляются тысячи маленьких файлов (особенно если данные приходят часто).  
Это убивает производительность: движок тратит время не на чтение данных, а на “открыть/закрыть миллион файлов” (I/O bound) и обработку метаданных.

Важный вывод: в lake важна дисциплина размера файлов, компактация и правила записи.

**Проблема 4: Изменение схем источников**  
Источники меняются постоянно. Если вы храните просто файлы:

- старые файлы с одной схемой,
    
- новые — с другой,
    
- и дальше каждый запрос превращается в “угадай схему”.
    

Без поддержки схемы на уровне метаданных поддержка становится ручной болью.

**Проблема 5: Трудно делать “инкрементальные обновления”**  
Когда данные — файлы, обновление выглядит как:

- перезаписать кусок файлов,
    
- следить, чтобы вы не перемешали старую и новую версии,
    
- не потерять согласованность.
    

Вывод, который должен прозвучать:  
Lake очень хорош как хранилище, но “просто S3 + parquet” не даёт уровня управляемости, который нужен под BI/продовую аналитику.

---

### 10. Появление концепции Lakehouse
    

Lakehouse появляется как ответ:  
“Мы хотим оставаться в дешёвом хранилище файлов, но работать с данными как с таблицами”.

Ключевая мысль, которую надо донести:  
Lakehouse — это **Lake + табличный слой метаданных**.

То есть:

- данные всё ещё лежат файлами в S3/MinIO,
    
- но появляется система, которая говорит:
    
    - “вот это таблица”,
        
    - “вот её схема”,
        
    - “вот какие файлы в неё входят”,
        
    - “вот какая версия актуальна”.
        

Что это даёт:

- понятные “табличные” операции поверх файлов,
    
- управление схемой,
    
- управляемые обновления,
    
- возможность понимать “актуальное состояние” таблицы даже если под капотом файлы менялись.
    

Здесь хорошо заходит формулировка:  
DWH заставляет данные быть табличными и строгими сразу.  
Lakehouse позволяет хранить в lake, но вести себя “как таблицы”.

---

11. Архитектура Lakehouse
    

Архитектурно lakehouse можно объяснить как 3 уровня:

**(1) Storage layer**  
Объектное хранилище (S3/MinIO). Там лежат файлы данных (обычно Parquet/ORC).

**(2) Table format / metadata layer**  
Это “мозг”: метаданные таблиц.  
Он знает:

- какие файлы составляют таблицу,
    
- какая схема,
    
- какие партиции,
    
- какие версии данных существуют,
    
- какие изменения были сделаны.
    

Важно подчеркнуть:  
Без этого слоя lakehouse не существует — это просто lake.

**(3) Compute engines**  
Движки, которые читают/пишут данные:

- Spark (чаще всего в учебных и продовых пайплайнах),
    
- Trino/Presto и аналоги,
    
- иногда — другие движки.
    

Идея в том, что:

- хранение отдельно,
    
- вычисления отдельно,
    
- а табличный слой склеивает это в понятную “таблицу”.
    

Ещё одна ключевая практическая вещь:  
lakehouse помогает организовать “табличные” операции вроде:

- “добавь новые данные инкрементально”,
    
- “перестрой партицию”,
    
- “измени схему”,  
    не превращая это в ад ручного менеджмента файлов.
---

### 12. Delta Lake и Apache Iceberg: решаемые задачи

Теперь конкретика: Delta Lake и Iceberg — это два самых известных “table format” для lakehouse. В лекции их лучше подавать не как “два продукта”, а как “класс решений”.

Что они решают (по сути):

**(A) Таблица как управляемый объект поверх файлов**  
Они дают таблице “паспорт”:

- где она лежит,
    
- какие файлы входят,
    
- какая схема,
    
- какая версия актуальна.
    

И это уже не “куча parquet”, а объект, с которым можно работать как с таблицей.

**(B) Версионирование данных (история изменений)**  
Таблица может иметь версии: что было “до”, что стало “после”.  
Это важно для:

- воспроизводимости,
    
- отладки,
    
- пересчётов,
    
- отката неудачных загрузок.
    

Важно объяснить это простым образом:  
“как git, только для таблицы”.

**(C) Эволюция схемы (schema evolution)**  
Источники меняются — добавляются колонки, меняются типы, появляются новые поля.  
Table format фиксирует изменения схемы как управляемый процесс, а не как хаос файлов.

**(D) Управление партициями и оптимизация хранения**  
Партиции могут меняться, таблица может оптимизироваться:

- бороться с мелкими файлами,
    
- делать компактацию,
    
- улучшать чтение запросов.
    

Это напрямую влияет на производительность, потому что в lakehouse почти всегда bottleneck — чтение файлов и метаданных.

**(E) Инкрементальные чтения / “прочитать только изменения”**  
Для пайплайнов важно уметь:

- не перечитывать весь датасет,
    
- а читать только “что добавилось/изменилось”.
    

В production это ключ к нормальным SLA и стоимости.

Как подать Delta vs Iceberg без холивара:  
Скажи, что в рамках курса достаточно понимать, что они оба дают “таблицы поверх S3”, а выбор зависит от стека компании и инструментов обработки. Тебе как преподавателю важно, чтобы студенты поняли **зачем нужен этот слой**, а не выучили “кто круче”.

---
### 13) Роль табличных форматов в аналитике

В аналитике основная стоимость почти всегда связана с чтением данных: сколько байт нужно прочитать, сколько раз распаковать, сколько колонок реально задействовано, сколько лишнего обработано. Табличные форматы для аналитики решают четыре задачи одновременно:

1. **Column pruning (чтение только нужных колонок)**  
    В BI/аналитике запросы редко используют все поля. Типичный запрос берёт 5–20 колонок из таблицы на 100–300 колонок. Колонковые форматы позволяют физически не читать ненужные колонки, резко уменьшая IO.
    
2. **Predicate pushdown (пропуск ненужных блоков по фильтрам)**  
    Запросы почти всегда фильтруют по датам, регионам, статусам, типам событий. В аналитических форматах хранятся статистики по блокам (min/max, null count и т.д.), что позволяет пропускать блоки, которые гарантированно не подходят под фильтр.
    
3. **Типизация и схема (schema + types)**  
    Аналитические пайплайны требуют корректных типов: даты, числа, decimal для денег, категориальные признаки, bool. Формат хранит типы внутри файла, снижая зависимость от “догадок” при чтении и облегчая совместимость между инструментами.
    
4. **Эффективное сжатие и кодировки (compression + encodings)**  
    Колонковые форматы умеют хорошо сжимать “похожие значения” в колонках и применять кодировки (словари, RLE, bit-packing), снижая размер на диске и ускоряя чтение (меньше байт → меньше IO).
    

Практический вывод: выбор формата часто влияет сильнее, чем выбор движка. Один и тот же датасет в CSV/JSON и в Parquet/ORC может отличаться по скорости чтения и стоимости обработки в разы, особенно при фильтрации по времени и работе с широкими таблицами.

---

### 14) Parquet и ORC: как устроены и почему они быстрые

**Общее: Parquet и ORC — колоночные форматы.** Их эффективность держится на трёх механизмах: (1) разделение по колонкам, (2) блоковая организация данных, (3) статистики по блокам.

#### Parquet (структура и механика)

- **Row Group** — основной блок данных по строкам. Внутри row group каждая колонка хранится отдельно.
    
- **Column Chunk** — данные одной колонки внутри row group.
    
- **Pages** — более мелкие блоки внутри column chunk (data pages, dictionary pages и др.).
    
- **Footer** — в конце файла хранится метаинформация: схема, список row groups, оффсеты, статистики.
    

**Почему это важно:**

- При запросе движок может читать только column chunks нужных колонок (column pruning).
    
- По статистикам в метаданных можно пропускать целые row groups, если фильтр не подходит (predicate pushdown).
    
- Размер row group влияет на производительность: слишком маленькие → много оверхеда; слишком большие → хуже параллелизм и “точность” пропусков по статистикам.
    

**Типичные кодировки Parquet:**

- Dictionary encoding (словари) — эффективно для категориальных полей.
    
- RLE (run-length encoding) — эффективно для повторяющихся значений.
    
- Bit-packing — эффективно для небольших целых.
    

**Компрессия Parquet:**

- Snappy/LZ4 — часто как баланс скорости/сжатия.
    
- ZSTD — часто даёт лучшее сжатие при хорошей скорости.
    
- Gzip — сильнее сжимает, но часто медленнее.
    

#### ORC (структура и механика)

- **Stripe** — основной крупный блок. Содержит данные по колонкам, индексы и статистики.
    
- **Row Index** — индекс внутри stripe, позволяющий более точно пропускать части данных.
    
- **Footer + Postscript** — метаданные, схема, статистики, информация о компрессии.
    

**Почему ORC быстрый:**

- Статистики и индексы встроены в формат и активно используются движками для пропусков.
    
- Часто лучше “заточен” под сценарии сканирования и агрегаций в экосистемах, где ORC исторически доминировал.
    

**ORC Bloom Filters (часто встречается)**

- Для некоторых колонок ORC может хранить bloom filters, ускоряя проверки “значение точно не встречается” при фильтрации (особенно на high-cardinality полях).
    

#### Parquet vs ORC (уровень курса)

- Оба — стандартные аналитические форматы.
    
- Оба дают column pruning + predicate pushdown + compression.
    
- В реальных стэках выбор часто определяется экосистемой: где удобнее писать/читать, какие инструменты используются, какие требования к совместимости.
    

---

### 15) Почему CSV и JSON плохо подходят для аналитики

CSV и JSON полезны как форматы обмена и “входные” форматы, но для аналитических хранилищ они создают системные проблемы:

1. **Нет колонкового чтения**  
    Чтобы прочитать одну колонку из CSV/JSON, движок всё равно вынужден парсить всю строку/объект.
    
2. **Дорогой парсинг**  
    CSV/JSON требуют преобразования строк в типы на лету (int, float, timestamp). Это CPU-затратно и плохо масштабируется на больших объёмах.
    
3. **Слабая типизация**  
    В CSV всё “строки”, в JSON типы могут быть нестрогими и изменчивыми. Это приводит к дрейфу схемы и ошибкам при объединении данных.
    
4. **Плохое сжатие и большой размер**  
    Текстовые форматы обычно заметно больше по объёму, чем Parquet/ORC с кодировками и колонковым сжатием.
    
5. **Сложно делать predicate pushdown**  
    Без встроенных статистик по блокам движок не может эффективно пропускать куски данных. Приходится сканировать гораздо больше.
    

Практический паттерн в платформах:  
CSV/JSON — на вход/RAW (или для обмена), Parquet/ORC — для хранения и вычислений (Staging и выше).

---

### 16) Schema evolution и изменения схем данных

В аналитических системах схемы меняются постоянно: появляются новые поля, переименовываются старые, меняются типы, уточняется смысл. Schema evolution — это набор практик и механизмов, которые позволяют переживать изменения схемы без разрушения пайплайнов и метрик.

#### Типовые изменения

- **Добавление колонок** (самый частый случай)
    
- **Удаление колонок** (часто заменяется “deprecated” подходом)
    
- **Переименование** (опасно без явного механизма маппинга)
    
- **Изменение типов** (string → int, int → decimal, timestamp precision и т.п.)
    
- **Изменение вложенной структуры** (если данные из JSON/структурированных событий)
    
- **Изменение партиционирования** (другая логика разбиения по датам/ключам)
    

#### Почему это ломает пайплайн без дисциплины

- Старые файлы и новые файлы могут иметь разные схемы.
    
- Читалка ожидает одну схему — получает другую.
    
- BI-метрики начинают считаться “по-разному” до и после изменения.
    
- Возникают скрытые ошибки: значения становятся NULL, поля смещаются, типы приводятся некорректно.
    

#### Практика управления схемой по слоям

- **RAW**: допускается наличие нестабильной схемы (данные “как пришли”), но обязательно хранить технические поля загрузки и источник.
    
- **Staging**: схема должна быть стабилизирована (типизация, единые названия, единые правила). Именно здесь фиксируется “контракт” данных.
    
- **Aggregated / витрины**: любые изменения схемы должны быть контролируемыми и согласованными, потому что на них завязаны BI и ML.
    

#### Подходы к изменениям типов

- Type promotion: расширение типа без потери (int → bigint, float → double, decimal расширение точности).
    
- Избегать “опасных” преобразований (string → int) без явной валидации.
    
- Для денег и финансовых метрик — использовать decimal-подход и фиксированные правила округления на одном уровне.
    

#### Роль метаданных и каталогов

Для управления схемами на уровне таблиц обычно используется табличный каталог (например, metastore-подобные решения) и/или lakehouse table formats (Iceberg/Delta). Каталог хранит “официальную” схему таблицы и помогает отличать “таблица” от “набора файлов”, а также согласовывать чтение между разными инструментами.

---

### 17) Сравнение DWH, Data Lake и Lakehouse

#### DWH (аналитическая БД / слой быстрых запросов)

- Сильные стороны: быстрые BI-запросы, стабильные схемы, удобная работа аналитиков, высокая предсказуемость.
    
- Слабые стороны: неудобно и дорого хранить “всё сырьё”, сложнее работать с нестабильными схемами и огромными архивами сырых данных.
    

#### Data Lake (файлы в S3/HDFS)

- Сильные стороны: дешёвое хранение больших объёмов, гибкость форматов, хранение истории и сырья, удобство пересчётов.
    
- Слабые стороны: без правил превращается в data swamp, сложнее обеспечить стабильные таблицы и удобство для BI.
    

#### Lakehouse (lake + табличный слой)

- Сильные стороны: сохраняет дешёвое хранение lake, добавляет управляемость таблиц (схемы/версии/метаданные), лучше переживает schema evolution, упрощает воспроизводимость и “табличное мышление” поверх S3.
    
- Слабые стороны: требует дисциплины метаданных, компактации, управления файлами, и всё равно часто дополняется DWH для максимально быстрых BI-витрин.
    

Практическая картина: архитектуры часто не взаимоисключающие. Lake хранит сырьё и промежутки, DWH/OLAP обслуживает быстрый consumption, lakehouse используется как управляемый слой таблиц поверх lake, если это нужно по требованиям и стеку.

---

### 18) Где хранить RAW, Staging и витрины

#### RAW

- Логичное место: Data Lake (S3/MinIO/HDFS).
    
- Причины: дешёвое хранение, масштабируемость, возможность пересчётов, хранение “как пришло”.
    
- Практики: партиционирование по дате загрузки, хранение исходного payload, технические поля ingestion.
    

#### Staging

- Часто тоже Data Lake (Parquet/ORC), иногда промежуточно в аналитической БД, если объёмы маленькие.
    
- Причины: Staging может быть большим, пересчитываться часто, и не всегда нужен для BI напрямую.
    
- Практики: стабильная схема, типизация, дедупликация, нормализация ключей, единые таймзоны/валюты.
    

#### Aggregated / DWH слой и витрины (Data Marts)

- Логичное место: аналитическая БД (ClickHouse/MPP/др.) или lakehouse-таблицы, если BI/SQL-движок работают поверх них эффективно.
    
- Причины: витрины нужны для быстрых запросов, простого SQL, стабильных метрик и BI-дашбордов.
    
- Практики: денормализация (широкие таблицы), предрасчёт метрик, оптимизация под чтение (партиции, сортировки, материализации).
    

Ключевое правило для методички:  
сырьё и полуфабрикаты — в lake; потребительские витрины под BI/ML — в слой, где запросы быстрые и схемы стабильные.

---

### 19) Связь архитектур хранения с лабораторными работами

Логика лабораторных обычно отражает реальную архитектуру:

- В начале студент получает сырые датасеты и формирует **RAW-слой** (lake-подобное хранение): сохранить, разложить по структуре, обеспечить воспроизводимость.
    
- Затем строится **Staging-слой**: очистка, типизация, нормализация, единые ключи и форматы времени/денег.
    
- Далее формируется **Aggregated/DWH и витрины**: метрики DAU, revenue, ретеншн, продуктовые срезы, таблицы для BI.
    
- После появления витрин выполняются аналитические задачи:
    
    - BI-дашборды работают по витринам;
        
    - ML for Analytics использует витрины/feature tables как вход.
        

Для закрепления связи “архитектура → практика” достаточно фиксировать на каждом шаге:

- где физически лежат данные (lake или DWH),
    
- в каком формате (CSV/JSON vs Parquet/ORC),
    
- кто потребитель (инженерный слой vs BI/ML),
    
- какая цель слоя (сохранить / привести в порядок / дать быстрый ответ).
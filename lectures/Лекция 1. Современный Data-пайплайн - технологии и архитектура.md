# Оперативный анализ данных
## Информация о курсе
![alt text](<attachments/Pasted image 20251210003842.png>)

## Описание курса
Курс предназначен прежде всего для таких специальностей как дата-аналитик, дата-инженер, ml-инженер, bi-аналитик, системный аналитик и нацелен на получение студентами практических навыков 
- проектирования и разработки ETL процессов
- анализ сформированной студентом витрины через BI инструменты
- реализация фичей для ML моделей, построенных на основе данных из созданной студентом витрины
(прим. - что лабы что лекции выстроены именно так, мы сначала строим витрину, потом анализируем и делаем мл модельки)
![alt text](<attachments/Pasted image 20251210010223.png>)
## Используемые технологии
В рамках курса студент изучит такие технологии как:
DWH: Airflow Spark Clickhouse MinIO dbt
BI: Apache Superset, Yandex Datalens
ML: pandas scikit-learn Linear Regression K-Means
![alt text](<attachments/Pasted image 20251210010617.png>)
В Big Data мире технологий больше, чем можно представить и у каждой компании свой стек для решения своих задач со своими конкретными бюджетами и запросами, поэтому чтобы не распылять внимание на все были выбраны самые используемые на момент 25 года технологии (прим. которые легко развернуть, тот же apache iceberg и apache hadoop популярны но касаться будем только теоритически тк для нормального деплоя у каждого студента потребуется вечность :) )

## Что такое Big Data и зачем вам оно надо

Когда вы работаете с обычной базой данных, у вас обычно есть понятная картина:  
одна система, одна СУБД, таблицы, SQL-запросы.  
Данные лежат в одном месте, вы делаете `SELECT`, получаете результат — всё логично.

Big Data начинается в тот момент, когда **эта модель перестаёт работать**.

Не потому что SQL плохой.  
А потому что **меняется сама природа данных**.

---

### Почему «обычной» БД перестаёт хватать

В реальных системах данные:

- приходят **постоянно**, а не раз в день;
    
- приходят **из разных источников**, а не из одной БД/API веб-сервиса;
    
- имеют **разный формат**: таблицы, JSON, логи, события;
    
- накапливаются **годами**, а не очищаются каждую неделю;
    
- используются **разными командами** с разными задачами (ML/BI/ad-hoc аналитика).
    

Пример: мобильное приложение.

Один и тот же пользователь:

- кликает кнопки - > события,
    
- совершает платежи - > транзакции,
    
- пишет в поддержку - > текст,

Бизнесу выгодно собирать эти данные чтобы:

- считать метрики (DAU, MAU и тд),
    
- строить отчёты,
    
- прогнозировать выручку / дальнейшее поведение пользователей и т.д.,
    
- обучать ML-модели для тех же целей.

Всё это помогает бизнесу принимать правильные решения и увеличивать выручку

Именно для этого и нужна Big Data

---

### Big Data — это не «очень много данных»

Важно:  
Big Data — это **не просто большой объём**.

Big Data — это ситуация, когда:
- данные **невозможно обработать одной машиной**,
    
- данные **невозможно хранить в одной таблице**,
    
- данные **нельзя пересчитать «просто запросом»**,
    
- данные **нужно уметь пересобирать и переиспользовать**.
    

То есть проблема не в размере,  
а в **сложности управления данными**.

---

### Главная идея Big Data

Ключевая мысль, которую нужно запомнить:

> Big Data — это не про технологии.  
> Big Data — это про **архитектуру работы с данными**.

Мы больше не думаем:
- «у меня есть таблица — что из неё посчитать?»

Мы начинаем думать:
- откуда пришли данные,
    
- в каком виде они хранятся,
    
- как их быстро и точно обработать,
    
- как сделать результат воспроизводимым (как обеспечить идемпотентность).
---
### Почему Big Data важно именно вам

Даже если вы:
- аналитик,
    
- ML-специалист,
    
- backend-разработчик,

вы **всё равно работаете с результатом Big Data-процессов**.

Если вы аналитик:
- ваши витрины кто-то должен построить

Если вы делаете ML:
- модель обучается на витринах и фичах, подготовленных заранее.    

Если вы разработчик:
- ваши сервисы генерируют данные,
    
- которые потом используются для аналитики и прогнозов.

Понимание Big Data — это понимание **откуда берутся данные, которым вы доверяете**.

---

### Что мы будем называть Big Data в этом курсе

В рамках курса Big Data — это:

- работа с **потоками данных**, а не одиночными таблицами;
    
- разделение данных на **слои** (RAW, Staging, Aggregated);
    
- построение **витрин**, а не «запросов под отчёт»;
    
- подготовка данных так, чтобы они:
    
    - подходили для BI,
        
    - подходили для ML,
        
    - могли быть пересчитаны.
        

Мы **не будем** уходить в абстрактные теории.  
Мы будем шаг за шагом строить **реальную data-платформу**.

---
### Итог
Big Data — это ответ на простой вопрос:

> Как превратить хаотичный поток сырых данных  
> в понятные, стабильные и полезные метрики и прогнозы.

С этого момента дальше в курсе мы будем говорить:

- как эти данные правильно хранить,
    
- как их обрабатывать,
     
- и как из них получать ценность.


---
![[Pasted image 20251227140551.png]]

## Примеры ETL процессов
первый модуль посвящен построению ETL процесса для преобразования сырых исходных данных в агрегированную витрину
![alt text](<attachments/Pasted image 20251210011602.png>)
## Исходные данные
В нашем случае исходными данными являются датасеты из открытых источников - 
Kaggle, HuggingFace, отдельные сайты с архивами (прим. можно как по вариантам сделать так и одинаковым всё)
Базы данных зачастую используются, когда надо уже очищенные данные сформировать в виде витрины
REST API запросы используются для передачи данных из веб-приложений, мобильных приложений и тд в хранилище сырых данных (RAW слой)

Пример- Вася Пупкин отправил Вове Пукину 10000руб - данные об этой транзакции и об обновленных данных счетов пользователей уходят не только в БД платформы с транзакциями (оно используется для других целей, для быстрого доступа к данным в частности), но и в RAW слой хранилища данных для последующей очистки, агрегации и построения витрины по этим данным

## RAW слой дата-платформы 
![alt text](<attachments/Pasted image 20251210013842.png>)
RAW слой представляет из себя набор неструктурированных неочищенных данных, зачастую для хранения таких данных используются такие инструменты как Apache Hadoop, S3 или отдельные схемы в БД созданные специально для таких данных

### Назначение RAW слоя

RAW слой — это **первая точка приземления данных** в дата-платформе.  
Его основная задача — **сохранить данные в максимально исходном виде**, без бизнес-логики и агрегаций.

Ключевой принцип RAW слоя:  
**данные должны быть сохранены полностью и без потерь**.

Если данные пришли с ошибками, дубликатами или в неудобном формате —  
это нормально для RAW слоя. Исправление происходит позже.

---

### Почему нельзя пропускать RAW слой

RAW слой нужен по нескольким причинам:

- возможность **пересчитать витрины** при изменении логики;
    
- возможность **отладки ошибок** в пайплайнах;
    
- защита от потери данных при сбоях;
    
- аудит и воспроизводимость расчётов;
    
- единый источник истины для всех последующих слоёв.
    

Без RAW слоя любые ошибки в обработке приводят к безвозвратной потере данных.

---

### Какие данные обычно хранятся в RAW слое

В RAW слой попадают:

- события из приложений (логи, клики, действия пользователей);
    
- выгрузки из OLTP-БД (PostgreSQL, MySQL и т.д.);
    
- данные из внешних API;

Часто данные хранятся:

- в формате JSON / CSV / Parquet;
    
- с минимальной обработкой;
    
- с добавлением только технических полей (ingestion time, source, дата загрузки в RAW слой).
    

---

### Структура хранения данных в RAW слое

Даже при отсутствии очистки данные в RAW слое **всегда структурируются по каталогам**.

Типичный подход:

- разделение по источникам;
    
- разделение по дате загрузки;
    
- иногда — по типу события.
    

Это позволяет:

- быстро находить нужные данные;
    
- ограничивать объём обработки;
    
- масштабировать пайплайны.
    

---

### RAW слой ≠ аналитический слой

Важно понимать:

RAW слой **не предназначен для аналитических запросов**.

- из него не считают DAU;
    
- из него не строят BI-дашборды;
    
- из него не обучают ML-модели напрямую.
    

RAW слой — это **архив и фундамент**, а не рабочая витрина.

---

### Краткий вывод

RAW слой — это:

- самый «грязный» слой,
    
- самый важный слой,
    
- слой, без которого невозможно построить надёжную data-платформу.  


Примеры:
S3 - это облачное объектное хранилище для файлов любого типа и объема, данные хранятся в бакетах, можно хранить как и обычные файлы(как картинки для вашего проекта по РИПу), так и табличные данные
Apache Hadoop — это фреймворк/платформа с открытым исходным кодом для распределённого хранения и обработки больших наборов данных (Big Data) с использованием кластеров компьютеров. Он разбивает данные и задачи на части, которые параллельно обрабатываются на множестве узлов, что позволяет быстро анализировать огромные объёмы информации. 
Архитектура Hadoop гораздо сложнее S3 и развернуть его не менее сложно, поэтому в нашем курсе мы ограничимся работой с S3

## Staging слой

### Staging слой дата-платформы

Staging слой — это слой **первичной обработки данных**, который находится между сырым RAW слоем и аналитическими витринами.  
Его основная задача — **превратить сырые данные в технически корректные и согласованные**, но **ещё без бизнес-агрегаций**.

Если RAW слой — это «как данные пришли»,  
то Staging слой — это «как данные должны выглядеть, чтобы с ними можно было дальше работать».

---

### Что происходит с данными в Staging слое

В Staging слое данные:

- очищаются от явных ошибок;
    
- приводятся к единым типам данных;
    
- нормализуются по структуре;
    
- приводятся к единому формату времени, валют, кодировок;
    
- дедуплицируются (очистка от дублей);
    
- обогащаются минимальными техническими полями (время, когда событие было занесено в staging слой, id загрузки и пр.).
    

---

### Очистка данных (data cleaning)

На этом этапе решаются базовые технические проблемы:

- пустые или некорректные значения;
    
- неверные типы (строка вместо числа, число вместо даты);
    
- дубли строк;
    
- битые записи;
    
- некорректные timestamp’ы.
    

Важно:  
в Staging слое мы **не «угадываем», что имел в виду бизнес**,  
мы просто делаем данные **корректными с технической точки зрения**.

---

### Нормализация данных — ключевая идея Staging слоя

Нормализация в контексте Staging слоя — это **приведение данных к логически правильной структуре**, а не обязательно строгие 3NF, как в классических СУБД.

Основные цели нормализации:

- устранить дублирующуюся информацию;
    
- разделить сложные вложенные структуры;
    
- сделать данные пригодными для соединений (JOIN).
    

---

### Пример нормализации

Допустим, в RAW слое есть событие поездки в формате JSON, где:

- данные пользователя,
    
- данные самоката,
    
- данные города
    

хранятся **в одной записи**.

В Staging слое:

- пользователь выделяется в отдельную таблицу;
    
- самокат — в отдельную;
    
- город — в отдельную;
    
- в таблице поездок остаются только ссылки (ID).
    

Это позволяет:

- избежать дублирования данных;
    
- обновлять справочники независимо;
    
- строить устойчивые витрины.
    

---

### Почему нормализация важна именно здесь

Если не нормализовать данные на этапе Staging:

- агрегированные витрины становятся тяжёлыми и хрупкими;
    
- любые изменения источников ломают аналитику;
    
- ML-фичи начинают считаться по разной логике;
    
- возрастает риск расхождения метрик.
    

Staging слой — это **точка, где структура данных «фиксируется»**.

---

### Что НЕ делается в Staging слое

Важно понимать ограничения:

В Staging слое **не**:

- считают DAU, MAU, revenue;
    
- объединяют данные под конкретный отчёт;
    
- добавляют продуктовую или финансовую логику;
    
- делают витрины под BI.
    

Staging слой — это **подготовка данных**, а не аналитика.

---
## Aggregated слой
Aggregated слой (его также называют **DWH слоем** или **аналитическим слоем**) — это уровень, на котором данные превращаются в **понятные бизнес-сущности и метрики**, пригодные для аналитики, BI и машинного обучения.

Если:
- RAW слой — «как данные пришли»,
    
- Staging слой — «данные приведены в порядок»,

то Aggregated слой — это **«что эти данные означают для бизнеса»**.

---

### Назначение Aggregated слоя

Основная задача Aggregated слоя — **ответить на бизнес-вопросы**, а не просто хранить данные.

Примеры таких вопросов:

- сколько у нас активных пользователей сегодня?
    
- какая выручка по дням и городам?
    
- как меняется поведение пользователей со временем?
    
- какие сегменты пользователей приносят больше всего дохода?
    

Именно здесь появляются:

- метрики (DAU, MAU, revenue, retention),
    
- аналитические витрины,
    
- данные для BI-дашбордов и ML-моделей.
    

---

### Что происходит с данными в Aggregated слое

В Aggregated слое данные:

- агрегируются (по дням, неделям, пользователям, регионам);
    
- объединяются между собой (JOIN фактов и измерений);
    
- обогащаются бизнес-логикой;
    
- приводятся к устойчивой аналитической модели;
    
- становятся стабильным источником для отчётов.
    

На этом этапе данные **перестают быть техническими** и становятся **аналитическими**.

---

### Структура данных в Aggregated слое

Как правило, Aggregated слой строится вокруг двух типов таблиц:

- **факт-таблицы** — события и измеряемые значения  
    (поездки, платежи, сессии, заказы);
    
- **таблицы-измерения** — справочники  
    (пользователи, продукты, города, тарифы, время).
    

Такой подход упрощает:

- написание аналитических запросов;
    
- построение витрин;
    
- поддержку и масштабирование модели данных.
    

---

### Пример Aggregated слоя

Факт-таблица поездок может содержать:

- идентификатор пользователя;
    
- идентификатор самоката;
    
- дату поездки;
    
- длительность;
    
- расстояние;
    
- стоимость;
    
- статус поездки.
    

Таблицы-измерения содержат:

- атрибуты пользователя;
    
- параметры самоката;
    
- информацию о городе;
    
- характеристики тарифа.
    

На основе этих таблиц можно легко считать DAU, revenue и другие метрики.

# Слой витрин
### Витринный слой (Data Marts)

Витринный слой — это **финальный слой дата-платформы**, предназначенный для **конкретных аналитических и бизнес-задач**.

Если:

- RAW — сохранить всё,
    
- Staging — привести данные в порядок,
    
- Aggregated / DWH — сформировать бизнес-сущности и базовые метрики,
    

то витринный слой — это **«данные под конкретные вопросы»**.

---

### Назначение витринного слоя

Основная задача витринного слоя —  
**сделать данные максимально удобными для использования**:

- BI-дашбордами,
    
- аналитическими SQL-запросами,
    
- ML-моделями,
    
- регулярной отчётностью.
    

Витрина отвечает не на абстрактный вопрос  
«какие у нас есть данные»,  
а на конкретный:

- DAU по дням,
    
- revenue по городам,
    
- retention по когортам,
    
- признаки для ML-модели.

И тд, метрик может быть очень много  

---

### Что такое витрина данных

Витрина — это:

- заранее рассчитанная таблица,
    
- с фиксированной бизнес-логикой   

Витрина всегда:

- отвечает на множество вопросов бизнеса
    
- оптимизирована под чтение,
    
- стабильна во времени.

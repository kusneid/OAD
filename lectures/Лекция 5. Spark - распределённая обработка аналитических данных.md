## Слайд 1. Зачем нужен **Spark** в аналитической обработке

### Суть концепции

**Spark** — распределённый движок вычислений для обработки больших объёмов табличных данных и их преобразований. Он решает задачу «посчитать/собрать результат из данных, которые слишком велики или слишком тяжёлые по операциям для одной машины», сохраняя при этом удобную модель работы через DataFrame и SQL-подобные операции.

### Как работает

Spark не «хранит данные» как СУБД. Он читает данные из внешнего хранилища (объектное хранилище, файловая система, таблицы) и выполняет вычисления параллельно на кластере. Базовая единица результата — новый набор данных (DataFrame), полученный из исходного через последовательность преобразований.

Три причины, почему «одна БД / один сервер» часто не закрывают аналитическую обработку:

- Полный пересчёт истории: нужно регулярно пересобирать производные слои/агрегации при изменении логики или при появлении поздних данных. В одной СУБД это упирается в время выполнения, ресурсы и конкуренцию с другими запросами.
    
- Тяжёлые преобразования: крупные join/groupBy, оконные вычисления, переработка сырых событий в структуру для аналитики. Это операции с большими объёмами шифтовки данных между потоками выполнения.
    
- Раздельное хранение и вычисления: данные лежат в файловых форматах (Parquet/ORC) и читаются пачками. Вычислительный слой масштабируется отдельно от хранения, что делает пересчёты контролируемыми по стоимости и времени.
    

### Проблемы и ограничения

Spark не является «ускорителем всего». Он особенно силён на операциях массового сканирования и преобразования данных, но:

- плохо переносит сценарии с миллионами мелких запросов (OLTP-подобное использование);
    
- чувствителен к размеру файлов, перекосу данных по ключам, сети и диску;
    
- требует дисциплины в проектировании преобразований, иначе расходы на shuffle и память делают работу непредсказуемой.
    

### Практические аспекты

Spark используют там, где нужно:

- построить или пересчитать слой данных на базе сырых событий;
    
- выполнить соединения больших наборов данных;
    
- построить агрегаты (дневные/часовые/по сущностям) для дальнейшего использования;
    
- готовить датасеты для последующей загрузки в serving-слой (например, в аналитическую БД).
    

### Связь с другими компонентами

Spark находится между хранением и потреблением: читает из объектного хранилища/RAW-Staging, формирует производные наборы (Staging/Aggregated/промежуточные артефакты), пишет обратно в файлы или в аналитическую БД. Оркестрация и расписания находятся вне Spark: Spark получает параметр периода/партиции и выполняет вычисление.

---

## Слайд 2. Место Spark в аналитической платформе: compute, не storage

### Суть концепции

Spark — это **compute layer**: слой, который выполняет вычисления над данными, но не является «источником истины» для хранения. Это разграничение задаёт правильную архитектуру: хранение должно быть устойчивым и дешёвым, вычисления — масштабируемыми и повторяемыми.

### Как работает

Типовая схема взаимодействия:

- чтение данных из файлового слоя (Parquet/ORC) по нужным партициям;
    
- преобразования (фильтрация, нормализация, джоины, агрегации);
    
- запись результата либо обратно в файловый слой, либо в аналитическую БД для быстрых запросов.
    

Spark эффективно работает с колонковыми форматами: он умеет читать только нужные колонки, применять predicate pushdown и использовать статистики файлов там, где они доступны. Это уменьшает объём чтения и снижает стоимость пересчётов.

### Проблемы и ограничения

Граница ответственности важна. Ошибки начинаются, когда Spark превращают в «всё в одном»:

- в Spark начинают хранить состояние «как в БД» и поддерживать его вручную;
    
- преобразования становятся неявными (разбросаны по задачам и скриптам без единой модели зависимостей);
    
- результаты публикуются без метаданных загрузки и без возможности воспроизведения.
    

### Практические аспекты

В реальной эксплуатации Spark чаще всего решает две задачи:

- «тяжёлая подготовка данных» (нормализация, дедуп, join, агрегации на больших объёмах);
    
- «пакетные пересчёты» (backfill диапазонов, пересборка артефактов при изменениях).
    

Spark редко является финальной точкой потребления: для BI и частых аналитических запросов обычно используют serving-слой с другим профилем нагрузки.

### Связь с другими компонентами

- Объектное хранилище: основной носитель больших объёмов файлов, из которых Spark читает и в которые пишет.
    
- Аналитическая БД: конечная точка для быстрых запросов и визуализации.
    
- Оркестрация: внешняя система задаёт расписание, период расчёта, параметры и ретраи; Spark выполняет вычисление детерминированно по входу.
    

---

## Слайд 3. Архитектура Spark: **Driver**, **Executors**, задачи и стадии

### Суть концепции

Spark-приложение — это распределённая программа, где управление и планирование выполняет **Driver**, а физические вычисления выполняют **Executors**. Понимание этой модели нужно, чтобы предсказывать поведение приложения: где возникают узкие места, почему падает память, почему «завис» stage.

### Как работает

**Driver**:

- строит логический план вычислений из пользовательского кода;
    
- оптимизирует план и превращает его в физический план;
    
- режет вычисление на стадии (**stages**) и задачи (**tasks**);
    
- раздаёт задачи executors и следит за прогрессом.
    

**Executor**:

- получает задачи, читает нужные части данных;
    
- выполняет вычисления (фильтры, проекции, агрегации, join);
    
- пишет shuffle-данные на диск/по сети, если требуется;
    
- возвращает частичные результаты или пишет итоговые файлы.
    

Планирование в Spark строится вокруг понятия **shuffle boundary**: границы, где данные надо перераспределить между executors по ключу. Между shuffle границами Spark выполняет узкие операции локально, внутри партиций.

### Проблемы и ограничения

- Узкое место Driver: слишком большой план, слишком много мелких задач, слишком много метаданных — Driver начинает тратить время и память на планирование, а executors простаивают.
    
- Узкое место Executor: недостаток памяти, сильный shuffle, disk spill, медленная сеть, перегруженный диск.
    
- «Долгий хвост» задач: несколько тасков выполняются сильно дольше остальных из-за перекоса данных, что задерживает завершение стадии.
    

### Практические аспекты

На практике важно уметь отвечать на вопросы:

- сколько стадий и почему именно столько;
    
- где появился shuffle и чем он вызван (join/groupBy/repartition);
    
- почему часть executors простаивает (перекос данных, неравномерные партиции);
    
- что именно потребляет память: кэш, shuffle, агрегации, join.
    

### Связь с другими компонентами

Spark запускается под управлением менеджера ресурсов (standalone/YARN/Kubernetes). Менеджер выделяет контейнеры/поды под executors, а Spark управляет задачами внутри выделенных ресурсов. Логи и метрики жизненно важны: без них невозможно понять, на какой стадии и по какой причине деградирует выполнение.

---

## Слайд 4. **DataFrame** в Spark: распределённая таблица со схемой

### Суть концепции

**DataFrame** — основная абстракция Spark для аналитических вычислений. Это распределённый набор строк с фиксированной **схемой (schema)**, над которым выполняются декларативные операции: select/filter/join/groupBy/window. В отличие от pandas DataFrame, данные физически разбиты на партиции и могут лежать на разных executor’ах.

### Как работает

DataFrame в Spark — это не «данные в памяти», а описание вычисления:

- ссылка на источник (файлы/таблицы/поток);
    
- набор преобразований, которые должны быть применены;
    
- схема, которую Spark выводит или получает явно.
    

DataFrame операции образуют дерево выражений. Spark собирает это дерево, применяет оптимизации и только затем исполняет. Благодаря этому можно писать код «как над таблицей», а Spark подбирает физический план.

Три ключевые характеристики DataFrame:

- **Schema-aware**: у колонок есть типы, что позволяет оптимизировать исполнение и избегать части ошибок.
    
- **Columnar execution** (где возможно): Spark старается исполнять операции по колонкам и использовать эффективное представление данных.
    
- **Immutability**: операции не изменяют DataFrame на месте; каждая операция создаёт новый DataFrame-объект (новое описание вычисления).
    

### Проблемы и ограничения

- Типы данных: неверно выведенная схема (например, строка вместо числа) приводит к ошибкам в агрегатах и join, либо к неожиданным null’ам.
    
- Сложные UDF: пользовательские функции на Python часто ломают оптимизации и резко замедляют исполнение, потому что выносят вычисление из оптимизированного движка в интерпретатор.
    
- «Скрытая стоимость» простых операций: groupBy/join выглядят коротко в коде, но чтобы их выполнить, Spark может перераспределить терабайты данных.
    

### Практические аспекты

DataFrame удобен тем, что:

- позволяет держать трансформации декларативными;
    
- даёт единый стиль для ETL: читать → преобразовать → писать;
    
- поддерживает контроль плана исполнения через explain и метрики.
    

### Связь с другими компонентами

DataFrame — мост между файловыми форматами и вычислением. Он позволяет читать Parquet/ORC со схемой, применять преобразования и записывать результат обратно в файловый слой или в аналитическую БД. Контроль схемы и типов — обязательная часть промышленного пайплайна, иначе downstream-слои начинают получать нестабильные данные.

---

## Слайд 5. Ленивые вычисления: **transformations**, **actions**, **DAG**

### Суть концепции

Spark использует **lazy evaluation**: преобразования на DataFrame не выполняются сразу. Они накапливаются в виде плана (DAG вычислений), а выполнение начинается только при вызове **action**. Это ключ к оптимизации: Spark видит всю цепочку операций и может перестроить её так, чтобы выполнить быстрее.

### Как работает

**Transformation** — операция, которая возвращает новый DataFrame и добавляет шаг в план:

- select, withColumn, filter, join, groupBy (до момента фактического выполнения).
    

**Action** — операция, которая требует реального результата:

- count, collect, write, show, take, foreachPartition.
    

DAG строится как граф зависимостей. Spark группирует операции в **stages**: последовательности вычислений без shuffle. При появлении операции, требующей перераспределения данных (обычно join/groupBy), Spark ставит границу shuffle и начинает новую стадию.

### Проблемы и ограничения

- Иллюзия «код уже посчитал»: пока не было action, ошибок чтения данных, ошибок типов, проблем сети/диска может не проявиться.
    
- Дорогие actions: count по огромному набору данных — это полный скан. show тоже может запустить значимое вычисление, если перед ним тяжёлые преобразования.
    
- Неконтролируемые collect: collect переносит данные на Driver. На больших объёмах это заканчивается OOM или перегрузкой сети.
    

### Практические аспекты

Два инструмента контроля:

- **explain()**: показывает логический и физический план, можно увидеть, где shuffle, какие join-стратегии, есть ли фильтры до чтения.
    
- кэширование (будет отдельный слайд): фиксирует промежуточный результат, чтобы не пересчитывать ветки DAG.
    

Минимальный пример для демонстрации разницы transformation/action:

```python
df2 = df.filter("event_type = 'click'").select("user_id", "event_time")  # transformation
df2.explain(True)  # анализ плана, не обязательно запускает полный расчёт
n = df2.count()    # action: запускает выполнение
```

### Связь с другими компонентами

Ленивость делает Spark предсказуемым в пайплайнах: весь расчёт — детерминированная функция входных данных и параметров запуска. Это упрощает повторные прогоны и пересчёты: один и тот же вход + один и тот же код дают один и тот же результат, если контроль источников и партиций выполнен корректно.

---
## Слайд 6. План выполнения: **logical plan**, **physical plan**, оптимизации Catalyst

### Суть концепции

Spark исполняет DataFrame-операции не «как написано в коде», а через два плана: **logical plan** (что нужно вычислить) и **physical plan** (как именно это будет выполнено на кластере). Оптимизации выполняются до запуска задач, поэтому две одинаковые по смыслу цепочки операций могут быть исполнены по-разному, если Spark смог применить больше оптимизаций.

### Как работает

Logical plan строится из выражений DataFrame/SQL: фильтры, проекции, join, агрегации. Далее **Catalyst optimizer** переписывает план, применяя набор правил:

- **predicate pushdown**: фильтры проталкиваются ближе к источнику данных, чтобы читать меньше строк;
    
- **column pruning**: читаются только нужные колонки из Parquet/ORC;
    
- перестановки и упрощения выражений: удаление лишних преобразований, объединение последовательных операций;
    
- выбор стратегии join и агрегации на основе статистик и конфигурации.
    

После этого Spark строит physical plan: набор стадий и задач, конкретные операторы (scan, filter, hash aggregate, sort merge join) и точки shuffle. На этом уровне становится видно, сколько будет shuffle, будет ли broadcast join, где появятся сортировки.

Практический способ увидеть планы — `explain`:

```python
df_transformed.explain(True)
```

### Проблемы и ограничения

Оптимизатор ограничен качеством статистик и схемы:

- если источники без статистик или с неверными типами, Spark выбирает консервативные стратегии, часто с лишним shuffle;
    
- Python UDF и некоторые нестандартные операции «отрывают» вычисления от оптимизатора, ухудшая план и производительность;
    
- отсутствие явной схемы при чтении (или «всё строка») делает оптимизации менее эффективными и добавляет стоимость кастов.
    

### Практические аспекты

Контроль плана — базовый навык, иначе производительность становится случайной:

- проверка, что фильтры и проекции стоят до тяжёлых операций;
    
- проверка join-стратегии (broadcast vs shuffle);
    
- проверка числа shuffle-стадий и причин их появления.
    

Две типовые конфигурации, которые часто проверяют в начале:

- включён ли **AQE (Adaptive Query Execution)**, чтобы Spark мог менять план по ходу выполнения:
    

```python
spark.conf.set("spark.sql.adaptive.enabled", "true")
```

- адекватно ли выставлено число shuffle-партиций (обсуждается на слайде 8).
    

### Связь с другими компонентами

План напрямую зависит от формата хранения и схемы: Parquet/ORC дают возможность pushdown и column pruning. Если данные лежат «плоскими» файлами без схемы, Spark вынужден читать больше и оптимизирует хуже. План также определяет нагрузку на сеть и диск кластера: каждый shuffle превращается в сетевой обмен и записи промежуточных файлов.

---

## Слайд 7. Узкие и широкие трансформации: **narrow dependency** vs **wide dependency**

### Суть концепции

Все трансформации в Spark делятся на узкие и широкие по типу зависимости между партициями. **Narrow**-операции обрабатываются внутри существующих партиций без перераспределения данных. **Wide**-операции требуют переразбиения данных по ключу и создают shuffle, что является самым дорогим классом операций в Spark.

### Как работает

DataFrame физически разбит на партиции. Каждая партиция обрабатывается отдельной задачей (task).

- Узкая зависимость: выходная партиция зависит от одной входной (или небольшой части одной). Примеры: `select`, `withColumn`, `filter`, `mapPartitions`.
    
- Широкая зависимость: выходная партиция зависит от множества входных, потому что данные нужно собрать по ключу. Примеры: `groupBy`, `join` (кроме broadcast), `distinct`, `repartition`.
    

Широкие операции разрезают вычисление на стадии. До wide операции Spark может выполнить целый «конвейер» операторов в одном stage без сетевого обмена. Wide операция создаёт shuffle boundary и запускает следующий stage с уже перераспределёнными данными.

### Проблемы и ограничения

- Ошибка проектирования: выполнение широких операций слишком рано. Если `join` или `groupBy` стоит до фильтра и проекции, Spark перегоняет по сети лишние колонки и строки.
    
- Слишком частые `repartition`: каждый такой шаг потенциально создаёт shuffle, даже если логика могла обойтись без него.
    
- Неявные wide операции: `distinct`, `dropDuplicates`, некоторые оконные вычисления выглядят «лёгкими», но по факту требуют перераспределения и часто сортировки.
    

### Практические аспекты

Управление партициями — инструмент контроля стоимости:

- **coalesce** уменьшает число партиций без полного shuffle (не всегда, но часто дешевле), полезен перед записью, чтобы уменьшить количество файлов;
    
- **repartition** создаёт полное перераспределение и используется, когда действительно нужно изменить распределение данных по ключу.
    

```python
df_small_files = df.coalesce(64)       # уменьшение партиций (обычно дешевле)
df_balanced    = df.repartition(200)   # перераспределение (shuffle)
```

Частый практический критерий: если операция логически требует «собрать вместе одинаковые ключи», будет wide dependency. Это означает сеть + диск + рост времени.

### Связь с другими компонентами

Wide операции создают нагрузку на сеть и локальные диски executors, потому что shuffle пишет и читает промежуточные файлы. Если инфраструктура слабая по диску или сети, даже корректная логика будет работать медленно. Поэтому архитектурный выбор «где выполняются join/агрегации» всегда связан с профилем инфраструктуры кластера.

---

## Слайд 8. Shuffle: что происходит внутри и почему это главный источник деградации

### Суть концепции

**Shuffle** — перераспределение данных между executors по ключу, необходимое для join, groupBy и ряда других операций. Он дорогой потому, что включает запись промежуточных данных на диск, сетевую передачу и последующее чтение этих данных для финального вычисления.

### Как работает

Shuffle обычно состоит из двух фаз:

1. **shuffle write**: задачи предыдущей стадии вычисляют частичные результаты и пишут их в набор файлов, разбитых по целевым партициям;
    
2. **shuffle read**: задачи следующей стадии читают куски этих файлов по сети с разных executors, собирают данные своей партиции, после чего выполняют агрегацию/join/сортировку.
    

Внутри shuffle часто возникает:

- **spill**: данные не помещаются в память, Spark сбрасывает их на диск;
    
- сортировка: для некоторых стратегий (например, sort-merge join) данные сортируются по ключу;
    
- рост количества файлов: каждая map-задача может писать множество выходных сегментов.
    

Ключевой параметр, влияющий на масштаб shuffle, — число shuffle-партиций:

```python
spark.conf.set("spark.sql.shuffle.partitions", "200")
```

### Проблемы и ограничения

- Неправильный размер shuffle-партиций: слишком мало — партиции огромные, задачи долгие и падают по памяти; слишком много — много мелких задач, накладные расходы на планирование и чтение.
    
- Disk spill и медленный локальный диск: при нехватке памяти shuffle превращается в дискозависимую задачу.
    
- Сеть: shuffle — это реальная нагрузка на межузловую сеть; при слабой сети время растёт непропорционально объёму данных.
    
- Перекос ключей (skew): небольшая часть ключей может собрать большую часть данных и создать «длинный хвост» задач (подробно будет отдельным слайдом дальше по плану).
    

### Практические аспекты

Методы уменьшения стоимости shuffle, которые применяются повсеместно:

- ранняя фильтрация и отбор колонок до shuffle;
    
- предварительная агрегация (частичная агрегация до shuffle там, где это возможно);
    
- broadcast join, когда одна из таблиц достаточно мала;
    
- контроль числа выходных файлов через coalesce перед записью.
    

Проверка shuffle обычно начинается с `explain(True)` и чтения физического плана: наличие `Exchange` операторов означает shuffle boundary.

### Связь с другими компонентами

Shuffle напрямую упирается в железо кластера: локальные диски executors, сеть, настройки контейнеров, объём памяти. Поэтому одинаковый код может работать по-разному на разных кластерах. Управление shuffle — это стык логики трансформаций и инфраструктуры.

---

## Слайд 9. Join в Spark: стратегии, стоимость, типовые ошибки

### Суть концепции

Join — одна из самых дорогих операций в аналитике, потому что может порождать shuffle, раздувать объём данных и резко увеличивать потребление памяти. В Spark join — это не «одна операция», а выбор одной из стратегий исполнения, зависящий от размеров данных, статистик и конфигурации.

### Как работает

Основные стратегии join в Spark:

- **Broadcast hash join**: маленькая таблица рассылается на все executors, большая читается и соединяется локально без shuffle по большой таблице.
    
- **Sort-merge join**: обе стороны перераспределяются по ключу (shuffle), сортируются и соединяются. Часто используется по умолчанию на больших таблицах.
    
- **Shuffle hash join**: обе стороны шифлятся по ключу и соединяются через хэш-структуры (используется реже, зависит от условий).
    

Принудительное включение broadcast join возможно через `broadcast()`:

```python
from pyspark.sql.functions import broadcast
df_joined = fact_df.join(broadcast(dim_df), "key")
```

Автовыбор broadcast контролируется порогом:

```python
spark.conf.set("spark.sql.autoBroadcastJoinThreshold", 50 * 1024 * 1024)
```

### Проблемы и ограничения

- Несовпадение типов ключей: `string` vs `long` приводит к пустым join или к дорогостоящим кастам в плане.
    
- Null в ключах: join может терять строки или создавать неожиданные результаты, если не определены правила.
    
- Дубликаты ключей: many-to-many join может взорвать кардинальность результата и память.
    
- Перекос по ключу: один «популярный» ключ создаёт огромную партицию и задерживает stage.
    
- Неправильный порядок операций: join до фильтра и проекции увеличивает объём данных, который будет шифлиться.
    

### Практические аспекты

Рабочая последовательность для стабильных join:

- привести типы ключей заранее в Staging-логике;
    
- убрать лишние колонки до join;
    
- обеспечить уникальность в измерениях, если ожидается many-to-one;
    
- использовать broadcast для малых справочников;
    
- проверять physical plan и наличие `BroadcastHashJoin` / `SortMergeJoin`.
    

Диагностика делается через:

- `explain(True)` для понимания стратегии;
    
- оценку размеров сторон join (через статистики или предварительные агрегаты по ключам);
    
- анализ количества строк до/после join для контроля раздувания.
    

### Связь с другими компонентами

Join связывает compute и storage: качество схемы в файловом слое (типы, партиции) и качество подготовки данных определяют, будет ли join стабильным. Плохая схема и отсутствие контроля ключей превращают join в самый дорогой элемент пайплайна и источник непредсказуемых падений.

---

## Слайд 10. Агрегации и окна: как Spark считает **groupBy** и почему окна могут быть дорогими

### Суть концепции

Агрегации — центральный тип аналитических вычислений. В Spark агрегации реализуются как многоступенчатый процесс с частичными результатами и shuffle. Оконные функции добавляют зависимость от порядка и часто требуют перераспределения и сортировки, что делает их существенно дороже простых groupBy.

### Как работает

Для `groupBy` Spark стремится делать частичную агрегацию:

- локально внутри партиции вычисляются промежуточные агрегаты;
    
- затем данные шифлятся по ключу;
    
- финальная агрегация собирает результат по ключам.
    

Это уменьшает объём shuffle: вместо сырых строк пересылаются частичные агрегаты (когда это возможно). Реальная эффективность зависит от кардинальности ключей и от функций агрегирования.

Пример агрегации:

```python
from pyspark.sql import functions as F

agg_df = df.groupBy("event_date").agg(
    F.count("*").alias("cnt"),
    F.sum("amount").alias("revenue"),
    F.approx_count_distinct("user_id").alias("approx_users")
)
```

Оконные функции добавляют два требования:

- данные нужно сгруппировать по partition key окна;
    
- внутри каждой группы требуется порядок (`orderBy`), что обычно приводит к сортировке и может создать большой расход памяти/диска.
    

```python
from pyspark.sql.window import Window
w = Window.partitionBy("user_id").orderBy("event_time").rowsBetween(-6, 0)
df2 = df.withColumn("rolling_sum", F.sum("amount").over(w))
```

### Проблемы и ограничения

- Высокая кардинальность ключей: много групп → большие хэш-таблицы → spill.
    
- Неправильный выбор точных distinct: точный `countDistinct` дорогой на больших объёмах, часто требует тяжёлых структур и shuffle; аппроксимации уменьшают стоимость ценой ошибки.
    
- Оконные функции на больших группах: длинные последовательности событий на один ключ приводят к долгим задачам и spill.
    
- Смешивание окон и join: часто создаёт каскад shuffle и сортировок.
    

### Практические аспекты

Стабильные паттерны:

- агрегировать как можно раньше, если это не ломает смысл результата;
    
- использовать approx-метрики там, где допустима погрешность;
    
- следить за размером групп: окна эффективны на разумных размерах групп, иначе требуется переосмысление логики (например, предварительное сжатие или изменение гранулярности).
    

Диагностика:

- наличие `Exchange` и `Sort` в physical plan;
    
- время стадий и дисковый spill;
    
- распределение размеров групп (косвенно через выборку и предварительные расчёты).
    

### Связь с другими компонентами

Агрегации и окна формируют большую часть нагрузки compute-слоя. Их стоимость определяется не только кодом, но и тем, как данные организованы в хранении: партиционирование, количество файлов, типы колонок. Хорошая организация входных данных уменьшает стоимость shuffle и сортировок и делает агрегации предсказуемыми по времени.

---
## Слайд 11. **Data Skew**: перекос данных и «длинный хвост» задач

### Суть концепции

**Data skew** — ситуация, когда распределение данных по ключу сильно неравномерное: небольшое число ключей содержит непропорционально большой объём строк. В Spark это приводит к тому, что одна или несколько shuffle-партиций становятся гигантскими, а соответствующие задачи выполняются значительно дольше остальных. В результате стадия ждёт «длинный хвост» задач, и кластер простаивает, хотя большая часть работы уже завершена.

### Как работает

Skew проявляется в операциях, которые группируют данные по ключу или требуют перераспределения по ключу:

- `groupBy(key)` собирает все строки одного ключа в одну логическую группу;
    
- `join` без broadcast обычно шифлит данные по ключу соединения.
    

Если один ключ встречается в 10–1000 раз чаще других, то:

- значительная часть строк попадёт в одну shuffle-партицию;
    
- один executor будет вынужден обработать огромный объём данных;
    
- возрастает вероятность **spill** на диск из-за переполнения памяти, сортировок и хэш-структур.
    

Spark может пытаться компенсировать часть проблем через **AQE (Adaptive Query Execution)** и skew-join оптимизации, но полностью это не решает, если перекос экстремальный.

### Проблемы и ограничения

- Skew редко виден на «средних» тестовых выборках, но проявляется на полном объёме.
    
- Ускорение кластера (больше executors/памяти) может почти не помогать: bottleneck остаётся в одной партиции.
    
- Skew часто связан не только с данными, но и с логикой: выбор ключа join, использование статусов/категорий с малым количеством значений, объединение событий с «популярными» идентификаторами.
    

### Практические аспекты

Три распространённых подхода борьбы со skew:

1. Включение адаптивного исполнения и skew-join (если используется):
    

```python
spark.conf.set("spark.sql.adaptive.enabled", "true")
spark.conf.set("spark.sql.adaptive.skewJoin.enabled", "true")
```

Это помогает, когда skew умеренный: Spark может разбивать крупные партиции на более мелкие подзадачи.

2. **Salting ключа** для join/агрегаций: искусственно «размазывать» горячий ключ на несколько значений.  
    Идея: добавить соль `salt` (например, 0..N-1) к ключу на большой стороне и продублировать малую сторону по соли, чтобы распределение стало равномернее.
    

```python
from pyspark.sql import functions as F

N = 16
fact_salted = fact_df.withColumn("salt", (F.rand()*N).cast("int"))
dim_expanded = dim_df.crossJoin(spark.range(0, N).withColumnRenamed("id", "salt"))

joined = fact_salted.join(dim_expanded, ["key", "salt"])
```

Это увеличивает объём данных (dim размножается), но снимает bottleneck с одной партиции.

3. Пересмотр ключей и предварительная агрегация: если join делается по ключу, который изначально «плохой», часто правильнее изменить гранулярность или агрегировать до join.
    

Диагностика skew:

- по Spark UI: одна/несколько задач в stage занимают значительно больше времени;
    
- по метрикам: сильный spill на диск в отдельных задачах;
    
- по данным: частоты ключей (даже выборочно) показывают «тяжёлые» значения.
    

### Связь с другими компонентами

Skew усиливается на слабой инфраструктуре (диск/сеть) и при неправильной организации данных (слишком крупные файлы без партиционирования/пре-агрегации). На уровне данных skew часто является отражением реального распределения событий, поэтому архитектурно его учитывают при проектировании ключей витрин и промежуточных агрегаций.

---

## Слайд 12. Партиционирование данных: чтение/запись, **partition pruning**, контроль количества файлов

### Суть концепции

Партиционирование — способ физически организовать данные в хранилище так, чтобы Spark мог читать только релевантную часть файлов и параллелить обработку предсказуемо. В файловых форматах (Parquet/ORC) партиционирование обычно реализуется через директории вида `col=value`. Это влияет на стоимость чтения, стоимость инкрементальных пересчётов и количество выходных файлов.

### Как работает

При записи Spark может распределять строки по значениям колонок партиционирования и сохранять их в отдельные директории:

```python
df.write.mode("overwrite").partitionBy("event_date").parquet("s3a://staging/events/")
```

При чтении Spark применяет **partition pruning**: если фильтр использует колонку партиции, Spark пропускает нерелевантные директории и не открывает лишние файлы:

```python
df = spark.read.parquet("s3a://staging/events/").filter("event_date >= '2025-12-01'")
```

### Проблемы и ограничения

- Слишком высокая кардинальность партиции (например, партиционирование по `user_id`) создаёт миллионы директорий/файлов и ломает метаданные/листинг.
    
- Слишком грубая партиция (например, только год) ухудшает pruning и заставляет читать слишком много данных.
    
- **Small files**: при большом числе executors и партиций Spark может написать тысячи мелких файлов, что резко ухудшает последующее чтение и листинг в объектном хранилище.
    

### Практические аспекты

Типовая практика в аналитике:

- Партиционировать по времени (день/час) и иногда по крупной категории (регион/тип события), если это соответствует основным фильтрам.
    
- Перед записью контролировать количество выходных файлов:
    
    - уменьшать число партиций через `coalesce` (без полного shuffle), если распределение уже приемлемое;
        
    - избегать бессмысленных `repartition` без ключа.
        

```python
out = df.coalesce(128)
out.write.partitionBy("event_date").parquet("s3a://agg/metrics/")
```

Проверка качества партиционирования:

- типовые запросы/трансформации должны читать только нужный диапазон;
    
- количество файлов на партицию должно быть ограничено и стабильным;
    
- перечень директорий не должен разрастаться до уровня, когда листинг становится bottleneck.
    

### Связь с другими компонентами

Партиционирование в файловом слое является основой для инкрементальных пересчётов и управляемого backfill: можно заменять/пересчитывать отдельные партиции без полного пересборки. В объектном хранилище small files и листинг часто становятся узким местом, поэтому партиционирование — это одновременно про модель данных и про эксплуатацию.

---

## Слайд 13. Инкрементальная обработка в Spark: чтение «дельты», безопасная перезапись партиций

### Суть концепции

Инкрементальная обработка в Spark — способ ограничить вычисления новым диапазоном данных и публиковать результат так, чтобы повторный запуск не создавал дубликаты и не ломал прошлые периоды. Это требует двух вещей: устойчивого критерия выбора входа и детерминированного способа публикации результата (обычно по партициям).

### Как работает

Базовый паттерн:

1. определить период обработки (например, `process_date` или диапазон);
    
2. прочитать только нужные партиции/файлы;
    
3. выполнить преобразования;
    
4. записать результат с контролируемой перезаписью целевых партиций.
    

Чтение по партиции:

```python
df = spark.read.parquet("s3a://staging/events/").filter(f"event_date = '{process_date}'")
```

Запись с перезаписью целевой партиции часто строят через режим `overwrite` + корректную организацию путей, либо через динамическую перезапись партиций (если поддерживается окружением):

```python
spark.conf.set("spark.sql.sources.partitionOverwriteMode", "dynamic")
df.write.mode("overwrite").partitionBy("event_date").parquet("s3a://agg/metrics/")
```

### Проблемы и ограничения

- Поздние данные: если источник присылает события задним числом, простой инкремент «только сегодня» теряет записи или оставляет некорректные агрегаты.
    
- Неидемпотентная публикация: запись «добавлением» (append) без ключевого контроля быстро приводит к дубликатам.
    
- Неверный выбор временной колонки: `event_time` и `ingestion_time` могут отличаться; инкремент должен быть согласован с тем, как потребители интерпретируют данные.
    

### Практические аспекты

Два распространённых решения поздних данных:

- **пересчёт скользящего окна**: каждый запуск пересчитывает последние N дней (например, 7–30), а затем перезаписывает соответствующие партиции;
    
- **отдельная логика корректировок**: поздние события записываются в отдельный слой и периодически «сливаются» в агрегаты через backfill.
    

Пример скользящего окна чтения:

```python
df = spark.read.parquet("s3a://staging/events/") \
    .filter("event_date >= date_sub(current_date(), 7)")
```

Критерий готовности инкремента:

- можно однозначно определить, какие партиции пересчитываются;
    
- повторный прогон за тот же период приводит к тому же результату;
    
- публикация результата атомарна на уровне партиции или логического набора файлов.
    

### Связь с другими компонентами

Инкремент в Spark опирается на партиционирование файлового слоя и на соглашения по публикации результатов. Оркестрация задаёт период и ретраи, но корректность обеспечивается именно логикой чтения/перезаписи. Serving-слой (аналитическая БД) часто загружается из этих же партиций, поэтому ошибка в инкременте распространяется дальше как «корректные» данные.

---

## Слайд 14. Память и кэширование: **cache/persist**, storage levels, когда кэш вреден

### Суть концепции

Spark производителен, когда минимизирует повторные чтения и повторные вычисления, но память — ограниченный ресурс. **cache/persist** позволяет сохранить промежуточный DataFrame, чтобы не пересчитывать ветку DAG. Это работает только если кэшируемый набор данных реально используется повторно и помещается в доступные ресурсы.

### Как работает

`cache()` — сокращение для `persist()` с уровнем хранения по умолчанию. `persist()` позволяет выбирать уровень хранения (память/диск/сериализация):

```python
from pyspark import StorageLevel

df.persist(StorageLevel.MEMORY_AND_DISK)
df.count()   # materialize: кэш реально заполняется только после action
```

Кэширование ленивое: пока нет action, ничего не закэшировано. После материализации Spark хранит партиции DataFrame в памяти executors (и/или на диске, если память не хватает).

### Проблемы и ограничения

- Кэширование большого набора данных приводит к вытеснению памяти, росту spill и деградации других операций.
    
- Кэширование без повторного использования — чистая потеря ресурсов: тратится память/диск, а вычисление не ускоряется.
    
- Сериализация: хранение в памяти может быть как «объектным», так и «сериализованным»; сериализованное меньше по памяти, но дороже по CPU на чтение.
    
- Driver vs executors: кэш живёт на executors. Перегрузка Driver отдельная история и не лечится cache.
    

### Практические аспекты

Когда кэш оправдан:

- один и тот же DataFrame используется в нескольких последующих вычислениях (ветвление DAG);
    
- промежуточный результат дорогой (join/shuffle), но его можно переиспользовать;
    
- размер результата контролируемый и не разрушает память.
    

Паттерны эксплуатации:

- всегда материализовать кэш осознанно (через действие) и проверять использование;
    
- освобождать ресурсы после использования:
    

```python
df.unpersist()
```

- предпочитать `persist(MEMORY_AND_DISK)` для больших наборов, чтобы избежать падений при нехватке памяти.
    

### Связь с другими компонентами

Кэширование связано с ресурсной моделью кластера: объём памяти executors и конкуренция задач определяют, будет ли кэш стабилен. При чтении из объектного хранилища кэш может существенно снизить повторные чтения, но при неправильном применении превращается в источник нестабильности и деградации shuffle-операций.

---

## Слайд 15. Типичные ошибки и анти-паттерны в Spark

### Суть концепции

Spark допускает написание «правильного по синтаксису» кода, который будет катастрофически дорог по ресурсам и времени. Анти-паттерны обычно связаны с попыткой использовать Spark как локальную библиотеку, с отсутствием контроля shuffle и с публикацией результатов без управления файлами и партициями.

### Как работает (как именно ломают выполнение)

Наиболее частые анти-паттерны:

1. `collect()` / `toPandas()` на больших данных  
    Данные переносятся на Driver, что приводит к OOM или к сетевой перегрузке.
    

```python
pdf = df.toPandas()  # допустимо только на реально малых данных
```

2. Неконтролируемые Python UDF  
    UDF выносит вычисления из оптимизированного движка и часто убивает векторизацию/пушдауны.  
    Предпочтение — встроенным функциям Spark SQL.
    
3. Join без понимания размеров и ключей  
    Many-to-many join раздувает объём данных и запускает тяжёлый shuffle.
    
4. Small files на выходе  
    Запись тысяч файлов в объектное хранилище ухудшает последующее чтение и листинг. Причина — слишком много партиций при записи и отсутствие контроля `coalesce`.
    
5. `repartition` без ключа «на всякий случай»  
    Создаёт shuffle и удорожает пайплайн без выигрыша.
    
6. Отсутствие `explain` и контроля плана  
    Shuffle и сортировки появляются неожиданно, а диагностика начинается после деградации.
    

### Проблемы и ограничения

- Ошибки проявляются только на полном объёме, а не на сэмпле.
    
- Оптимизации «не спасают» плохой план: Spark может выбрать лучший из доступных вариантов, но не отменит фундаментальную стоимость shuffle и раздувание кардинальности.
    
- Исправления часто требуют изменения логики, а не только тюнинга конфигов.
    

### Практические аспекты

Базовый набор правил, который удерживает пайплайн в предсказуемых рамках:

- все операции, которые могут «взорваться», проверяются планом (`explain(True)`) и оценкой размеров;
    
- фильтры и проекции стоят до join/groupBy;
    
- broadcast используется для малых справочников;
    
- запись контролируется по числу файлов и партиций;
    
- кэш используется только для реального переиспользования.
    

### Связь с другими компонентами

Анти-паттерны часто связаны с тем, как организовано хранение: количество файлов, партиционирование, типы колонок. Плохая организация данных в объектном хранилище увеличивает стоимость чтения и shuffle, а неправильная публикация результатов делает downstream-слои медленными независимо от их собственного движка.

---
